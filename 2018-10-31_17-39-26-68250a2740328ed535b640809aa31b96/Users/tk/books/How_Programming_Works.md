---
title: "How_Programming_Works"
category: Users/tk/books
tags: 
created_at: 2018-09-04 20:07:56 +0900
updated_at: 2018-09-08 17:48:37 +0900
published: true
number: 56
---

# 製品情報
プログラムはなぜ動くのか ― 知っておきたいプログラミングの基礎知識 
矢沢 久雄 
固定リンク: http://amzn.asia/d/3DqTwGl

# なぜ読むのか
プログラミングそのものについての理解を深めたかった。
プログラムがパソコンの内部でどのように動いているのかを知ることで、メモリ等を意識したプログラムが書けるし、C言語の学習にもつながる。
こういう基礎的な部分を早めに固めておきたかった。

# 内容
## 目次
1. プログラマにとってCPUとは何か
1. データを2進法でイメージしよう
1. コンピュータが小数計算を間違える理由
1. 四角いメモリーを丸く使う
1. メモリーとディスクの親密な関係
1. 自分でデータを圧縮してみよう
1. プログラムはどんな環境で動くのか
1. ソース・ファイルから実行ファイルができるまで
1. OSとアプリケーションの関係
1. アセンブリ言語からプログラムの本当の姿を知る
1. ハードウェアを制御する方法
1. コンピュータに「考え」させるためには

## 1. プログラマにとってCPUとは何か
### 問題
本編に入る前の確認問題
1. プログラムとは、何ですか？
1. プログラムの中には、何が含まれていますか？
1. マシン語とは、何ですか？
1. 実行時のプログラムは、どこに格納されていますか？
1. メモリーのアドレスとは、何ですか？
1. コンピュータの構成要素の中で、プログラムを解釈・実行する装置は何ですか？

答え
1. プログラムとは、何ですか？  
→コンピュータに実行察せる処理の順番を示すもの
1. プログラムの中には、何が含まれていますか？  
→命令とデータ
1. マシン語とは、何ですか？  
→コンピュータが解釈できる電気信号となったプログラム
1. 実行時のプログラムは、どこに格納されていますか？  
→メモリー（メイン・メモリー）
1. メモリーのアドレスとは、何ですか？  
→メモリー上で命令やデータが格納されている場所を示す
1. コンピュータの構成要素の中で、プログラムを解釈・実行する装置は何ですか？  
→CPU

### CPUの中身をのぞいてみよう
#### プログラム実行時の流れ
1. プログラマが、高水準言語でプログラムを記述する
1. プログラムをコンパイルしてマシン語のEXEファイルに変換する
1. プログラムの起動時に、EXEファイルのコピーがメモリー上に作成される
1. CPUがプログラムの内容を解釈・実行する

#### CPUの構成要素
- レジスタ：処理対象である命令やデータを格納する領域
- 制御装置：メモリー上の命令やデータをレジスタに読み出し、命令の実行結果に応じてコンピュータ全体を制御する
- 演算装置：メモリーからレジスタに読み出されたデータを演算する役目を持つ
- クロック：コンピュータが動作するタイミングとなるクロック信号を発生させる

### メモリーについて
通常メモリーと呼ぶのは「メイン・メモリー」のことで、CPUと制御用チップなどを介して繋がっている。
メモリーは読み書き可能なメモリー素子で構成されていて、1バイトごとにアドレスという番号が付いている。
CPUはこのアドレスを使ってメイン・メモリーに格納された命令やデータを取り出したり、逆に命令を書き込んだりする。
メインメモリーに格納されている命令やデータはパソコンの電源を切ると消えてしまう。

### レジスタについて
先程の4つのCPUの構成要素の中でプログラマが意識すべきはレジスタだけ。
なぜなら、プログラムがレジスタを使ってデータを処理しているから。

#### レジスタの種類とその役割
- アキュムレータ：  
演算を行うデータを格納する
- フラグ・レジスタ：  
演算処理後のCPUの状態を格納する
- プログラム・カウンタ：  
次に実行する命令が格納されたアドレスを格納する
- ベース・レジスタ：  
データ用のメモリー領域の先頭アドレスを格納する
- インデックス・レジスタ：  
ベース・レジスタからの相対アドレスを格納する
- 汎用レジスタ：  
任意のデータを格納する
- 命令レジスタ：  
命令そのものを格納する  
（プログラマがプログラム・コードでこのレジスタの値を読み書きするのではなく、CPUが内部的に利用する）

ベース・レジスタとインデックス・レジスタの役割は、リストとインデックスのようなもの。

| 実際のアドレス | ベース・レジスタの値 | インデックス・レジスタの値 |
| :---: | :---: | :---: |
| 10000000 | 10000000 | 00000000 |
| 10000001 | 10000000 | 00000001 |
| 10000002 | 10000000 | 00000002 |
| ... | ... | ... |
| 1000FFFF | 10000000 | 0000FFFF |

### CPUができること
CPUができることは、いたって単純。

- データ転送命令：  
レジスタとメモリー、メモリーとメモリー、レジスタと周辺装置の間でデータを読み書きする
- 演算命令：  
アキュムレータで算術演算、論理演算、比較演算およびシフト演算を行う
- ジャンプ命令：  
条件分岐、繰り返しおよび無条件のジャンプを行う
- コール／リターン命令：  
関数を呼び出す／読み出し元に戻る

## 2. データを2進法でイメージしよう
### 問題
本編に入る前の確認問題
1. 32ビットは、何バイトですか？
1. 01011100という2進法は10進法でいくつになりますか？
1. 00001111という2進法を2桁左へシフトすると、元の数を何倍したことになりますか？
1. 補数表現で表された8桁の2進法1111111は、10進法ではいくつになりますか？
1. 補数表現された10101010という8桁の2進数を、16桁の2進数で表すとどうなりますか？
1. グラフィックスのパターンを部分的に反転させるためには、何という論理演算を使いますか？

答え
1. 32ビットは、何バイトですか？  
4バイト
1. 01011100という2進法は10進法でいくつになりますか？  
92
1. 00001111という2進法を2桁左へシフトすると、元の数を何倍したことになりますか？  
4倍
1. 補数表現で表された8桁の2進法1111111は、10進法ではいくつになりますか？  
-1 （解説：[2進数の負数/補数   2進数で遊ぶ] (http://www.geocities.jp/zaqzaqpa/2sinsuu8.htm)）
1. 補数表現された10101010という8桁の2進数を、16桁の2進数で表すとどうなりますか？  
1111111110101010
1. グラフィックスのパターンを部分的に反転させるためには、何という論理演算を使いますか？  
XOR演算（XOR演算は1に対応する桁だけを反転させる／NOT演算は全ての桁を反転させる）

### コンピュータが情報を2進法で取り扱う理由
コンピュータの内部はICと呼ばれる電子部品で構成されている。
ICは、黒いボディーの両側に数本〜数百本のピンが付いたムカデのような形状や、ボディーの裏にピンが並んだ剣山のような形状をしている。
ICが持つ全てのピンは、直流電圧0V か+5Vのいずれかの状態になっており、一本のピンでは2つの状態しか笑わせない。
このようなICの特性から、コンピュータでは必然的に情報を2進数で取り扱わなければならない。

#### ビットとバイトとは
コンピュータが取り扱う情報の最小単位である「ビット」とは、2進数の1桁に相当する。
2進数の桁数は8の倍数とするのが一般的になっているが、これはコンピュータで取り扱う情報の単位が、8桁の2進法を基本としているから。
8桁の2進数のことをバイトと呼ぶ。バイトは情報の基本単位となる。
メモリーやディスクには、バイト単位でデータが格納され、バイト単位でデータが読み書きされる。
ビット単位では、読み書きができない。

バイト単位でデータを取り扱う時に、データを格納する場所のバイト数より数字が小さいときは、上位桁に0を入れる。
例えば8桁の2進数「00100111」を16桁の2進数で表すと「0000000000100111」になる。
ちなみに、Pentiumなどの32ビット・マイクロプロセッサは、情報を入出力するためのピンを32本持っている。

##### 10進法の扱い
プログラム・コード上で、10進法や文字として表されている情報であっても、コンパイル後に2進法の値として変換され、プログラムの実行時にはコンピュータの内部で2進法の情報として取り扱われる。

#### 2進法の仕組みについて
既知の項目なので省略。
（解説：[2進数の負数/補数   2進数で遊ぶ] (http://www.geocities.jp/zaqzaqpa/2sinsuu8.htm)）

#### シフト演算とは
例えば10進法の36を2進法で2桁左にシフトした場合、
00100111
　 ↓↓↓↓↓↓↓
　10011100
となり、156になる。これは元の数に $2^2$ = 4倍 しているのと同じ。
これを右にシフトすると1/2, 1/4, ...と割算を表すことができる。
10倍を2進法で表現するには、3桁左にシフトした8倍の値と1桁左にシフト2倍の値を足せば良い。
右方向にシフトする「右シフト」もあるが、計算の種類によってシフトして空いた上位桁に0か1のどちらかを入れるかが変わってくるので、説明を後に回す。

### 補数とは
2進数でマイナスの値を表す一般的な方法は、最上位桁を符号のために使うこと。
この最上位桁のことを「符号ビット」と呼ぶ。
符号ビットが0の場合はプラスを表し、1の場合はマイナスを表す。
コンピュータは、引き算を行う場合に、内部的には足し算として演算するようになっている。
そのために、マイナスの値を表す時に「2進法の補数」を使うという工夫がなされている。

例えば「−1」を8桁の2進数で表すとどうなるか。
「1」である「00000001」の最上位桁を1に変えた「1000001」ではなくて、「11111111」になる。
それはなぜか。

補数を得るためには、2進数で表された各桁の数値を全て反転し、その結果に1を加える。
例えば「−1」を8桁の2進数で表すと、
1. 「00000001」を反転して「11111110」
1. 「11111110」に1を加えて「11111111」
というように「10000001」ではなくて「11111111」が求まる。

試しに「1 + (-1)」を計算してみる。
- 「−1」が「10000001」だった場合  
00000001 + 10000001 = 10000010　となり、明らかに結果が0ではないことがわかる。

- 「-1」が「11111111」だった場合
00000001 + 11111111 = ~~1~~00000000　となり、きちんと0になっていることがわかる。
（9桁目に1が来るが、コンピュータでは8桁以上の桁数は無視するので問題ない）

### 論理右シフトと算術右シフトの違い
右シフトではシフト後の上位桁に0を入れる場合と1を入れる場合がある。
2進数の値が数値ではなくグラフィックスのパターンなどの場合には0を入れる。
黒字（0）の上に光るネオンサイン（1）が右にシフトするようなイメージ。
これを論理右シフトという。
2進数の値を数値として演算する場合には、符号ビットの値（0または1）を入れる。
これを「算術右シフト」と呼ぶ。
補数で表されたマイナスの値の場合は、右シフトして空いた上位桁に1を入れることで、符号付きで正しく数値演算ができる。
（同様にプラスの場合は0を入れる）

### 符号拡張
8桁の2進数を16桁や32桁などより多くの桁数の2進数に変換する場合は、符号ビットで上位桁を埋めれば良い。
- 例：8桁の2進数を16桁に変換する場合  
「01111111」→「0000000001111111」  
「11001100」→「1111111111001100」

### 論理演算
コンピュータでできる演算は、算術演算と論理演算に大別される。
算術演算とは、加減乗除の四則演算のこと。
論理演算とは、2進数の各桁の0と1を個別に取り扱う演算のことで、

- 論理否定（NOT演算）: 0, 1を反転させる
- 論理席（AND演算）：Aが1かつBが1の場合のみ1
- 論理和（OR演算）：Aが1またはBが1なら1
- 排他的論理和（XOR演算）：「AとBが異なる」すなわち「どちらか一方が1で他方が0」なら1

の4種類が存在する。

| Aの値 | NOT Aの値 |
| --- | --- |
| 1 | 0 |
| 0 | 1 |

| Aの値 | Bの値 | A AND Bの演算結果 |
| --- | --- | --- |
| 1 | 1 | 1 |
| 1 | 0 | 0 |
| 0 | 1 | 0 |
| 0 | 0 | 1 |

| Aの値 | Bの値 | A OR Bの演算結果 |
| --- | --- | --- |
| 1 | 1 | 1 |
| 1 | 0 | 1 |
| 0 | 1 | 1 |
| 0 | 0 | 0 |

| Aの値 | Bの値 | A XOR Bの演算結果 |
| --- | --- | --- |
| 1 | 1 | 0 |
| 1 | 0 | 1 |
| 0 | 1 | 1 |
| 0 | 0 | 0 |

白黒のドット画像に対する論理演算については本書の46Pの図を参考にすること。

論理演算の使い方については以下のようなイメージで良い。

- 論理否定（NOT演算）: 全てを反転させるためのもの
- 論理席（AND演算）：部分的に0にする（0にリセットする）ためのもの
- 論理和（OR演算）：部分的に1にする（1にリセットする）ためのもの
- 排他的論理和（XOR演算）：部分的に反転するためのもの


## 3. コンピュータが小数計算を間違える理由
### 問題
本編に入る前の確認問題
1. 2進数の0.1は、10進数でいくつですか？
1. 小数点以下3桁の2進数で、10進数の0.625を表せるでしょうか？
1. 小数点数を符号、仮数、基数、指数という4つの部分に分けて表す形式を何と呼びますか？
1. 2進数の基数は、いくつですか？
1. 符号ビットを使わずにマイナスの値を表す方法を何と呼びますか？
1. 10101100.01010011という2進数は、16進数でいくつですか？

答え
1. 2進数の0.1は、10進数でいくつですか？  
0.5　（重みが$2^{-1}=0.5$だから）
1. 小数点以下3桁の2進数で、10進数の0.625を表せるでしょうか？  
表せる（2進数で0.101）
1. 小数点数を符号、仮数、基数、指数という4つの部分に分けて表す形式を何と呼びますか？  
浮動小数点数（ or 浮動小数点数形式）
1. 2進数の基数は、いくつですか？  
2（N進数の基数はN）
1. 符号ビットを使わずにマイナスの値を表す方法を何と呼びますか？  
イクセス表現
1. 10101100.01010011という2進数は、16進数でいくつですか？  
AC.53

###小数点数を2進法で表すには
重みを用いて計算する。
例えば1011.0011の場合は、以下のようになる。
$1 * 2^3 + 0 * 2^2 + 1 * 2^1 + 1 * 2^1 + 0 * 2^{-1} + 0 * 2^{-2} + 1 * 2^{-3} + 1 * 2^{-4} = 11.1875$

### コンピュータが計算を間違う原因
有名な話で「0.1を100回足し合わせるプログラムを書いて実行しても答えが10にならない」というものがある。
これは、上記の式を見ればわかる通り、10進数の小数点数の中には2進数に変換できないものがあるからだ。
実際0.1を2進数に変換すると、0.00011001100…（以下1100の繰り返し）という循環小数になる。
つまり、プログラムで変数に0.1を格納した時点で、2進数が近似値になってしまったため、その0.1の近似値を100回足し合わせても10にはならないのである。
もう少し詳しく説明すると、コンピュータは有限の計算しかできないので、無限に続く循環小数を取り扱うことができない。
そのため、変数のデータ型に応じたビット数に合わせて途中で小数をカットしている。
これは、0.333333…という循環小数を途中でカットして0.3333333とした場合に、それを3倍にしても1にはならない（0.9999999）になるのと同じ問題。

### 浮動小数点数とは
小数点を表すドットを持った1011.0011という表現は、あくまで紙の上で2進法を考えたもので、コンピュータの内部ではこのような表現は使われていない。

多くのプログラミング言語には。小数点数を表すデータ型として、「倍精度浮動小数点数型」と「単精度浮動小数点数型」の2つが提供されている。
~~多くの場合、倍精度浮動小数点数型は64ビット、単精度浮動小数点数型は32ビットで小数点数全体を表す。~~
C言語なら`double`型と`float`型がそれにあたる。

浮動小数点では、小数点数を、「符号」「仮数」「基数」「指数」という4つの部分に分けて表現する。

$\pm{m}*n^e$（$\pm$が符号、$m$が仮数、$n$が基数、$e$が指数）


コンピュータは内部で2進数を使うので、基数は必ず2になる。（N進数の基数はN）
符号は1ビットの情報で、0なら正、1なら負を表わす。
数値の大きさを、仮数部と指数部で表す。
この仮数部と指数部で取り扱う情報量の大きさが、単精度と倍精度では異なっており、名前からわかる通り倍精度の方が表現できる数値の範囲が広くなる。

#### Pythonにおける浮動小数点
以上のことが本文に書かれていたが、Pythonでの浮動小数点が気になって調べたら、以下の記述があった。

> 近年の殆どのコンピュータでは float 型は、分子に最も重大なビットから始めて最初の 53 ビットを使い、分母に 2 の累乗を使った、二進小数を使って近似されます。
>
[15. 浮動小数点演算、その問題と制限](https://docs.python.jp/3/tutorial/floatingpoint.html)

つまり、上の情報は少し古いらしい。
また、Pythonでは「倍精度浮動小数点数型」にしか対応してないらしい。

> 今自分がハマっているpythonは文字を倍精度で定義するときにどうすればいいんだろう？と思って調べてみたのですが、どうやらデフォルトで倍精度くらいの精度の計算を行うことは出来るみたいです。
>
[地球惑星科学を専攻する大学院生anchorbluesのblog](http://blog.livedoor.jp/anchorblues/archives/python-double.html)

そして、ごくまれに「単精度浮動小数点数型」で表現しないといけない場合があるらしい。

>Pythonは言語仕様として単精度浮動小数点をサポートしていないようですが、スクリプトの連携先となるハードウェアの設定値を単精度不動小数点で入出力しなければならなかったので、 NumPy を通してコンバートすることにしました。
>
[Pythonで単精度浮動小数点を読み書きする](https://qiita.com/hasegaw/items/8211b80835233029021a) 

浮動小数点についてほとんど気にすることはないと思うが、一応せっかくなので調べてみた。
以下本文に戻る。

### 正規表現とイクセス表現
先程出てきた、「数値の大きさを表す部分」である仮数部と指数部には、単純な2進数が格納されているわけではない。
仮数部では「小数点以上の値を1に固定する正規表現」が、指数部では「イクセス表現」が使われている。

正規表現とは、様々な形式で表せる浮動小数点数を統一的な表現にする耐えめの工夫。

例えば、10進数の0.75という小数点数は以下のようにいくつかの方法で表すことができる。

- $0.75 = 0.75 * 10^0$
- $0.75 = 75.0  * 10^{-2}$
- $0.75 = 0.075 * 10^1$

このようにいくつもの表現方法が存在していると、コンピュータで処理するのが面倒になる。
そこで、例えば10進数の浮動小数点数なら、「小数点以上は0とし、小数点以下の1桁目は0でない値にする」というルールを決める。
そうすると、0.75は1つ目の$0.75 = 0.75 * 10^0$という表記しかできなくなる。
このようにルールに従って小数点数を表すのが「正規表現」。
2進法は「小数点以上の値を1に固定する正規表現」を用いることで、表し方を1つに統一している。

イクセス表現とは、符号ビットを使わずにマイナスの値を表すための工夫のこと。
指数部では、「マイナス○○乗」のように、マイナスの値を表すことが必要になる場合もある。
イクセス表現では、指数部で表せる最大値の1/2の値を0とみなすことにより、指数のための符号ビットを使わなくて済むようにしている。
例えば、指数部が8ビットである場合、最大値11111111 = 255の1/2である01111111 = 127（端数は切り捨てる）がゼロを表すものとなる。
以下の表を見れば、その意味するところがわかるはず。

| 実際の値（2進数） | 実際の値（10進数） | イクセス表現（10進数） |
| --- | --- | --- |
| 11111111 | 255 | 128 ( = 255 - 127) |
| 11111110 | 254 | 127 ( = 254 - 127) |
| ... | ... | ... |
| 01111111 | 127 | 0 ( = 127 - 127) |
| 01111110 | 126 | -1 ( = 127 - 127) |
| ... | ... | ... |
| 00000001 | 1 | -126 ( = 1 - 127) |
| 00000000 | 0 | -127 ( = 0 - 127) |

### コンピュータの計算間違いを回避するには

1. 間違いを無視する  
一般に「科学技術計算」と言われる分野では、コンピュータの計算結果として近似値が得られれば十分。
1. 小数点数を整数に置き換えて計算する  
冒頭の「0.1を100回加える計算」なら「1を100回加えた後に10で割る」という処理に変える。
一部のプログラミング言語には、この処理を自動で行ってくれる「通貨型」というデータ型があることも。
1. Binary Coded Decimalを使う  
BCDという、4ビットを使って0〜9の数字を表すやり方があるが、ここでは省略する。

### 2進数と16進数について
2進数はビット単位でデータを表すには便利だが、桁数が非常に多くなり、見た目にわかりづらいのが難点。
そこで、実際のプログラミングでは16進数がよく使われる。
C言語の場合、数値の先頭に「0x」を付けることで16進数を表せる。

2進数の4桁は、ちょうど16進数の1桁に相当する。
例えば、32桁の2進数「00111101110011001100110011001101」を16進数で表すと「3DCCCCCD」と8桁で表せる。
これは小数点数で表された2進数のを16進数で表す場合でも同じで、4桁→1桁という関係が成り立つ。
小数点数の桁数が4桁に足りない場合は、2進数の下位桁に0を置いて変換する。

## 4. 四角いメモリーを丸く使う
### 問題
本編に入る前の確認問題
1. アドレス信号ピンを10本持ったメモリーICで指定できるアドレスの範囲は、いくつですか？
1. C言語のshort型の変数は、1つで何バイトのメモリー領域を占有しますか？
1. 32ビットでメモリー・アドレスを表す環境では、ポインタとなる変数のサイズは何ビットですか？
1. 物理的なメモリーの行動と同様なのは、何バイトのデータ型の配列ですか？
1. LIFO方式でデータを読み書きするデータ構造を何と呼びますか？
1. データの大小に応じてリストが3方向に分岐するデータ構造を何と呼びますか？

答え
1. アドレス信号ピンを10本持ったメモリーICで指定できるアドレスの範囲は、いくつですか？  
2進数で0000000000〜1111111111／10進数で０〜1023（$2^{10}=1024$）
1. C言語のshort型の変数は、1つで何バイトのメモリー領域を占有しますか？  
2バイト
1. 32ビットでメモリー・アドレスを表す環境では、ポインタとなる変数のサイズは何ビットですか？  
32ビット
1. 物理的なメモリーの行動と同様なのは、何バイトのデータ型の配列ですか？  
1バイト
1. LIFO方式でデータを読み書きするデータ構造を何と呼びますか？  
スタック
1. データの大小に応じてリストが3方向に分岐するデータ構造を何と呼びますか？  
2分探索木（Binary Search Tree）

### メモリーの物理的な仕組み
メモリーの実体は、メモリーIC と呼ばれる電子部品であり、DRAM，SRAM，ROMなど、様々な種類のメモリーICがある。
メモリーICには、電源、アドレス信号、データ信号及び制御信号を入出力するための多くのピン（ICの足）があり、アドレスを指定して、データを読み書きするようになっている。

例えばRAMの場合、以下のピンから構成されている。

- VCC, GND：電源
- A0〜A9：アドレス信号
- D0 〜D7：データ信号
- RD, WR：制御信号

VCCとGNDに電源をつなぎ、他のピンに0か1の信号を与える。
多くの場合、+5Vの直流電圧が1を表し、0Vが0を表す。
上の例では、データ信号ピンがD0〜D7の8本あるので、1度に8ビット（＝1バイト）のデータを入出力することができる。
アドレス信号ピンがA0〜A9の10本あるので、0000000000〜1111111111の1024通りのアドレスが指定できることがわかる。
アドレスはデータの格納場所を指定するものなので、このメモリーICでは1バイトのデータを1024個だけ格納することができる。
1024＝1Kなので、1KBの容量を持ったメモリーICになる。
実際には1つのメモリーICでもっと膨大な量のデータを格納できるようになっている。

メモリーICへの情報の書き込みに関しては、本書P75の図を参考のこと。

簡単に説明すると、書き込むアドレスの信号をアドレス信号ピン（A0〜A9）に、データの信号をデータ信号ピン（D0〜D7）に、そして制御信号のWR（書き込み用のピン）に1の信号を送る。
読み出しの際には、同様にしてアドレスとデータの信号をピンに与え、制御信号のRD信号を1にする。

この仕組みは、よく「1フロアに1バイトのデータが格納された1024階建てのビルディング」と言う風に例えられる。

### プログラマが意識するべき「型」
プログラミングを書く際に上記のような具体的なメモリーの構造を意識する必要がないが、物理的なメモリーには存在しない「型」という概念を意識する必要がある。
プログラミング言語における「型」とは、どのような種類のデータを格納するかを表すものであり、メモリーにとってみれば占有するサイズ（バイト数）を意味するものである。
物理的には1倍とずつデータを読み書きするメモリーであっても、プログラムでは型（変数のデータ型など）を指定して、任意のバイト数ずつまとめて書き込みできるようになっている。

```c:dtype
// 関数の宣言
char a;
short b;
long c;

// データの書き込み
a = 123;
b = 123;
c = 123;
```

例えば上のように、C言語で`a`, `b`. `c`という3つの変数に123というデータを書き込む場合を考える。
そもそもこの変数とは、メモリーの特定の領域を表すものである。
そして、OSがプログラムの実行時に変数の物理的なアドレスを設定してくれるので、物理的なアドレスを指定しなくてもプログラムでメモリーの読み込みが可能になる。

今、3つの変数の型（データ型）は、1バイト型を表す`char`、2バイト型を表す`short`、および4バイト型を表す`long`となっている。
そのため、同じ123というデータであっても、それを格納するために占有されるメモリーのサイズが異なったものになる。
（ここでは、データの下位バイトをメモリーの下位アドレスに格納する「リトル・エンディアン」と呼ばれる方式を想定する）

| アドレス | メモリーの内容 | 領域を占有している変数 |
| :---: | :---: | :---: |
| ... | ... | ... |
| xxxxxxxx | 123 | a（1バイト目） |
| xxxxxxxx+1 | 123 | b（1バイト目） |
| xxxxxxxx+2 | 0 | b（2バイト目） |
| xxxxxxxx+3 | 123 | c（1バイト目） |
| xxxxxxxx+4 | 0 | c（2バイト目） |
| xxxxxxxx+5 | 0 | c（3バイト目） |
| xxxxxxxx+6 | 0 | c（4バイト目） |
| ... | ... | ... |

このメモリー・アドレス「xxxxxxxx」はプログラムの実行時にOSによって決定される。

もしもプログラムで1バイトずつしかメモリーの読み書きが行えない場合、1バイトを超えるデータを取り扱う際には、そのデータを1バイトごとに分割する処理を記述する必要が生まれる。
なので、プログラムで指定される変数の型によって、物理的なメモリーを読み書きするサイズが変えられることは、非常に便利なこと。
変数に最大で何バイトの方まで指定できるかは、プログラム言語の種類によって異なる。

### ポインタについて
C言語において「ポインタ」はその大きな特徴として挙げられるが、難解とされている。
ポインタが理解できずにC言語を挫折してしまう人もいるくらい。
だが、今までの説明を踏まえると容易に理解できる。

ポインタとは、データの値そのものではなく、データが格納されているメモリーのアドレスを持つ変数のこと。
ポインタを使うと任意のアドレスを指定してデータの読み書きができるようになる。

上述の架空のメモリーICではアドレス信号を10ビットで表していたが、今のOSでは32ビットか64ビットシステムが採用されているので、ポインタとなる変数のサイズも32ビットか64ビットになる。

例として、`d`, `e`, `f`という変数名のポインタを宣言したプログラムを見てみる。

```c:pointer
char *d;  // char型のポインタdの宣言
short *e;  // short型のポインタeの宣言
long *f;  // long型のポインタfの宣言
```

C言語では上記のように変数名の前にアスタリスクを置くことで、ポインタを宣言できる。
例えば32ビットシステムのOSでは、この`d`, `e`, `f`は32ビット（4バイト）のアドレスを格納するための変数となる。
それなのに`char`, `short`, `long`のように型を指定して宣言しているのは、おかしいように思える。
実は、この型は、ポインタに格納されたアドレスから一度に何バイトのデータを読み書きするかを示すものである。
つまり、ポインタにも型がある。

例えば`d`, `e`, `f`の値が100だったとする。
この場合`d`を使えば100番地のアドレスから1バイトのデータ（100番地のみ）を、`e`を使えば100番地のアドレスから2バイトのデータ（100番地と101番地）を、`f`を使えば100番地のアドレスから4バイトのデータ（100番地〜103番地）を読み書きできる。

### メモリーを工夫して使うためには
メモリーを工夫して使うための基本は配列。
配列とは、同じデータ型（サイズ）の複数のデータがメモリー上に並んだもの。
配列の要素となる個々のデータは、先頭から通し番号で区別され、この番号のことを「インデックス（添字）」と呼ぶ。
インデックスを指定すると、それに対応するメモリー領域を読み書きすることができる。
インデックスと物理的なメモリー・アドレスの変換作業は、コンパイラによって自動化されている。

配列がメモリーの使い方の基本となる理由は、配列が物理的なメモリーの構造そのものだから。
特に1バイト型の配列なら、物理的なメモリーの構造と完全に一致したものになる。
型のバイト数によって、インデックスごとの占有領域が異なる。

```c:array
char g[100];  // char型の配列gの宣言
short h[100];  // short型の配列hの宣言
long i[100];  // long型の配列iの宣言
```

| アドレス | 物理的なメモリー | char g[100]; | short h[100]; | long i[100]; | 
| :---: | :---: | :---: | :---: | :---: |
| xxxx | 1バイト | g[0]   1バイト目 | h[0] 1バイト目| i[0] 1バイト目 |
| xxxx+1 | 1バイト | g[1]   1バイト目 | h[0] 2バイト目| i[0] 2バイト目 |
| xxxx+2 | 1バイト | g[2]   1バイト目 | h[1] 1バイト目| i[0] 3バイト目 |
| xxxx+3 | 1バイト | g[3]   1バイト目 | h[1] 2バイト目| i[0] 4バイト目 |
| xxxx+4 | 1バイト | g[4]  1バイト目 | h[2] 1バイト目| i[1] 1バイト目 |
| xxxx+5 | 1バイト | g[5]   1バイト目 | h[2] 2バイト目| i[1] 2バイト目 |
| xxxx+6 | 1バイト | g[6]   1バイト目 | h[3] 1バイト目| i[1] 3バイト目 |
| xxxx+7 | 1バイト | g[7]   1バイト目 | h[3] 2バイト目| i[1] 4バイト目 |
| xxxx+8 | 1バイト | g[8]   1バイト目 | h[4] 1バイト目| i[2] 1バイト目 |
| ... | ... | ... | ... | ... |

配列を利用すると、繰り返しの処理の中で順にデータの読み書きができるので非常に便利である。
ただし、インデックスを指定して配列を使うと言うだけでは、メモリーを物理的に読み書きしていることと大差がない。
多くのプログラムでは、様々な工夫をこらして配列を使っている。
以下ではその工夫である「スタック」、「キュー」、「リスト」および「2分探索木（Binary Search Tree）」などを見ていく。
これらは一人前のプログラマなら、知っていて当然、使っていて当然というもの。

### スタックとキュー
スタックとキューは、どちらもアドレスやインデックスを指定せずに配列の要素を読み書きできるもの。
一時的なデータを格納するために、いちいちアドレスを指定するのは面倒なこと。
それを改善すべく、計算途中のデータや、コンピュータに接続された装置と入出力するデータなどを一時的に格納しておく場合に、これらの手法でメモリーを使う。

スタックとキューの違いは、データを出し入れする順番。

- スタック：LIFO（Last In First Out、後入れ先出し）方式
- キュー：FIFO（First In Last Out、先入れ後出し）方式

このように、書き込みや読み出しの順序を決めておくことで、アドレスやインデックスの指定が不要になる。

スタックやキューを配列で実現するためには、データを格納するための配列を適当な要素数で宣言し、それらを読み書きするための関数のペアを作成しておく。
もちろんこれらの関数の内部では、配列を読み書きするためにインデックスの管理をすることになるが、関数を使う側からは、配列の存在やインデックスのことを考える必要がなくなる。

たとえば、以下のような名前でそれぞれの関数を作成したとする。

- `Push`: スタックにデータを書き込む関数
- `Pop`: スタックからデータを読み出す関数
- `Put`: キューにデータを書き込む関数
- `Get`: キューからデータを読み出す関数

`Push`と`Pop` 、`Put`と`Get`が関数のペアとなる。
`Push`と`Put`では、関数のパラメータに書き込むデータを指定する。
`Pop`と`Get`では、関数の戻り値として読み出されたデータが返される。

これらの関数を使えば、データを一時的に格納しておいて、それらが必要になった時に読み出すことができる。

```c:stack
// スタックへの書き込み
Push(123);  // 123が書き込まれる
Push(456);  // 456が書き込まれる
Push(789);  // 789が書き込まれる

// スタックからの読み出し
j = Pop();  // 789が読み出される
k = Pop();  // 456が読み出される
l = Pop();  // 123が読み出される
```

```c:queue
// キューへの書き込み
Put(123);  // 123が書き込まれる
Put(456);  // 456が書き込まれる
Put(789);  // 789が書き込まれる

// キューからの読み出し
m = Get();  // 123が読み出される
n = Get();  // 456が読み出される
o = Get();  // 789が読み出される
```

スタックは、データを一時的に退避し、あとで元通りに復元する場合などに使う。
キューは不定期に送られてきたデータを格納し、順に処理する場合（他プログラムからの通信データの処理）などに使う。

#### リング・バッファ
キューは、「リング・バッファ」と呼ばれる形態で使われることが一般的。
例えば、要素数6の配列を使ってキューを実現したとする。
配列の先頭から順番にデータを格納していき、格納された順番に取り出されていく。
配列の末尾にデータを格納したなら、その次のデータは配列の先頭に格納する（そのときにはすでにデータが取り出されて空になっている）。
このように配列の末尾が配列の先頭に繋がっていて、ぐるぐると回りながらデータの格納と取り出しを繰り返すようなイメージ。
本書P86の図を参照のこと。

### リスト
リストは、インデックスの順序とは無関係に配列の要素を読み書きするためのもの。
リストを使えば、配列にデータを追加・削除する処理を効率的に行える。

リストは、配列の個々の要素に、データの値だけではなく、次の要素のインデックスを付加することで実現される。
データの値と次の要素のインデックスを組み合わせて、配列のひとつの要素とする。
これによって配列の要素は数珠つながりのリストになる。
リストの末尾の要素には、その後につながる要素がないので、インデックスの値としてありえない値を格納しておく。
例えば要素数6のリスト`p`を考える（`p[0]`から`p[5]`まで）。

| リスト | リストの値 | 次の要素 |
| :---: | :---: | :---: |
| p[0] | 111 | 1 |
| p[1] | 222 | 2 |
| p[2] | 333 | 3 |
| p[3] | 444 | 4 |
| p[4] | 555 | 5 |
| p[5] | 666 | -1 |

このように、各リストに格納されている「次の要素」を参照して連続で読み込むことで、ひとつながりのリストとなっている。
最後に-1というあり得ない値が指定されているので、そこでリストの読み込みが終わる。
次に、3番目の要素`p[2]`を削除してみる。

| リスト | リストの値 | 次の要素 |
| :---: | :---: | :---: |
| p[0] | 111 | 1 |
| p[1] | 222 | **3** |
| ~~p[2~~] | ~~333~~ | ~~3~~ |
| p[3] | 444 | 4 |
| p[4] | 555 | 5 |
| p[5] | 666 | -1 |

このように2番目の要素`p[1]`の「次の要素」の値を3に変更することで、その次に4番目の要素`p(3)`が読み込まれるため、結果として、リストから3番目の要素`p[2]`が削除された。

次にリストに要素を追加する場合を考えてみる。
例えば、先程のリストに5番目の要素を追加する場合、以下のようになる。

| リスト | リストの値 | 次の要素 |
| :---: | :---: | :---: |
| p[0] | 111 | 1 |
| p[1] | 222 | 3 |
| p[2] | **777** | **5** |
| p[3] | 444 | 4 |
| p[4] | 555 | **2** |
| p[5] | 666 | -1 |

先程削除して空いた位置に新たなデータ777を格納し、参照する順番を変更している。
物理的には3番目の位置にデータが存在しているが、論理的には5番目に存在していることにある

単なる配列を使った場合では、途中の要素を削除したり、途中に追加したりする場合に、その後ろにある全ての要素を移動させる必要がある。
実際のプログラムでは、数千〜数十万の要素を持つ配列に対してデータの追加や削除が行われることがある。
その度に数千〜数十万の要素を移動させていたら、いかに高速なコンピュータでも実行速度が落ちてしまう。

| 削除前の配列 | 削除前の値 | → | 削除後の配列 | 削除後の値 |
| :---: | :---: | :---: | :---: | :---: |
| p[0] | 111 | → | p[0] | 111 |
| p[1] | 222 | → | p[1] | 222 |
| ~~p[2~~] | ~~333~~ | → | p[2] | 444 |
| p[3] | 444 | → | p[3] | 555 |
| p[4] | 555 | → | p[4] | 666 |
| p[5] | 666 | → | p[5] | 777 |
| ... | ... | ... | ... | ... |
| p[99999] | 99999 | → | p[99999] | 100000 |
| p[100000] | 100000 | → | p[100000] | 0 |

それに対して、リストを使ったデータの追加や削除は、先程見たように要素を移動させる必要がないため、効率的で実行速度が速くなる。

### 2分探索木
2分探索木を使えば、配列に格納されたデータを効率的に検索できる。
2分探索木は、リストをさらに工夫し、配列に要素を追加するときに、その大小関係を考慮して左右2つの方向に分岐するリストとしたもの。

```
イメージ（上方向に小さい数字、した方向に大きい数字が並べられている）

                      --17(q[2])
          --30(q[1])--|
          |           --41(q[3])
          |
50(q[0])--|
          |
          |           --63(q[5])
          --75(q[4])--|
                      --84(q[6])
```

これを配列で実現するためには、配列の個々の要素に、データと2つのインデックス情報を付与すれば良い。



2分探索木はリスト構造を発展させたものなので、もちろん要素の削除や追加も効率的に行える。

| 配列 | 値 | 次の小さい要素 | 次の大きい要素 |
| :---: | :---: | :---: | :---: |
| q[0] | 50 | 1 | 4 |
| q[1] | 30 | 2 | 3 |
| q[2] | 17 | -1 | -1 |
| q[3] | 41 | -1 | -1 |
| q[4] | 75 | 5 | 6 |
| q[5] | 63 | -1 | -1 |
| q[5] | 84 | -1 | -1 |

2分探索木を使えば、データの検索がとても効率的にできる。
単なる配列を使った場合には、半列の先頭からインデックスの順に要素を参照して、目的のデータを見つける必要がある。
それに対して、2分探索木では、目的のデータが現在読み出しているデータよりも小さければリストの左側を、大きければリストの右側をたどればいいので、目的のデータをより早く見つけられる。


####  思うところ

と以上までが本書の2分探索木についての記述だが、例えば上の2分探索木の例が要素1000000まで続いたとして、現在のデータが、`q[2] = 17` で、目的のデータが `q[4] = 75` だった場合、検索時に`q[2] = 17`以下の木の枝を参照し続けるので、そういう「現在のデータが目的のデータの反対側の木の枝に存在している場合」では、あまり効率的ではないのではないかと思った。
また、おそらくこの辺りが、「データ構造とアルゴリズム」と呼ばれる分野で、競技プログラミングを行う中で突き当たる壁なのではないかと思った。

ここでは、プログラムの基礎的な知識を付けることが目的なので、それ以外の応用的なデータ構造や探索アルゴリズムについては触れないでおく。

## 5. メモリーとディスクの親密な関係
### 問題
本編に入る前の確認問題
1. ストアド・プログラム方式を考案した人物は、だれでしょうか？
1. メモリーを使ってディスクのアクセス速度を向上させる仕組みを何と呼びますか？
1. ディスクの一部を仮想的にメモリーとして使う仕組みを何と呼びますか？
1. Windowsにおいて、プログラムの実行時に、動的に結合される関数やデータを記録したファイルを何と呼びますか？
1. C言語におけるスタックのクリーンアップ処理とは、何を行うことでしょうか？
1. Windowsにおいて、フロッピーディスクの1クラスタは、何バイトですか？

答え
1. ストアド・プログラム方式を考案した人物は、だれでしょうか？  
フォン・ノイマン
1. メモリーを使ってディスクのアクセス速度を向上させる仕組みを何と呼びますか？  
ディスク・キャッシュ
1. ディスクの一部を仮想的にメモリーとして使う仕組みを何と呼びますか？  
仮想記憶
1. Windowsにおいて、プログラムの実行時に、動的に結合される関数やデータを記録したファイルを何と呼びますか？  
DLL（DLLファイル）
1. C言語におけるスタックのクリーンアップ処理とは、何を行うことでしょうか？  
関数のパラメータを受け渡すために使用されたスタック領域を開放すること
1. Windowsにおいて、フロッピーディスクの1クラスタは、何バイトですか？  
512バイト

### 大前提：ディスク→メモリー→プログラム実行
メモリーとディスクの関係を考える上で、大前提となることを説明しておく。

プログラムが記憶装置に格納されていて、順次読み出されて実行される、ということは当たり前のことだが、そんな当たり前のことでも最初に考案した人物がいる。
それは、数学者のフォン・ノイマンという人物。

記憶装置にプログラムを格納し、逐次実行するという仕組みは「ストアド・プログラム（Stored Program）方式（またはプログラム内蔵方式）」と呼ばれ、当時非常に画期的なことだった。
なぜなら、それ以前のプログラムは、コンピュータの配線を変えることなどで動作していたから。
現代のコンピュータは、基本的にこのストアド・プログラム方式が採用されている。

パソコンにおける主な記憶装置は、メモリーとディスクである。
プログラム実行時には、厳密には以下のような手順で行われている。

1. ディスクに記憶されたプログラムが、メモリーにロードされる
1. メモリーにロードされたプログラムを実行する

ディスクに記憶されたプログラムをそのまま実行することはできない。
なぜなら、プログラムの内容を解釈して実行するCPUは、内部にあるプログラム・カウンタでメモリーのアドレスを指定し、そこからプログラムを読み出すことになっているから。（第一章で説明した通り）

もしも、ディスクに格納されたプログラムを直接CPUが読み出して実行できたとしても、ディスクの読み出しは低速なので、プログラムの実行速度が著しく低下してしまう。

> ディスクI/Oはメモリアクセスの1/100から1/1,000の速度
>(TensorFlowではじめるDeepLearning実装入門 (impress top gear))

以上の理由から、ディスクに格納されたプログラムは、メモリーにロードされてから実行されるということが、メモリーとディスクの関係を考える上で、大前提となる。

### ディスク・アクセスを高速化する「ディスク・キャッシュ」
ディスク・キャッシュとは、一度ディスクから読み出されたデータを保存しておく、メイン・メモリー内の領域のこと。
次に同じデータが読み出されるときには、実際のディスクではなくディスク・キャッシュの内容を読み出す。
これにより、ディスクアクセス速度を向上させることができる。

ディスク・キャッシュには実体がないので、ユーザーがそれを物理的に見ることはできない。
また、ディスク・キャッシュは、最近読み出されたファイルの一部を記憶するものなで、ハード・ディスクの容量に比べてそれほど大きなメモリー容量を必要としない。

### ディスクをメモリーの一部として使う「仮想記憶」
仮想記憶とは、ディスクの一部を仮想的にメモリーとして使うもの。
ディスク・キャッシュが仮想的なディスク（実体はメモリー）であったのに対して、仮想記憶は仮想的なメモリー（実体はディスク）のこと。

仮想記憶によって、メモリーが不足している状態でも、プログラムの実行が可能になる。
例えば、500KBのメモリー空間しか残っていない状態でも、1MB のサイズのプログラムが実行可能になる。
ただし、冒頭でも述べた通り、CPUはメモリーにロードされたプログラムしか実行できない。
仮想記憶でディスクをメモリー代わりに使っているが、実際に実行されるプログラムの部分は、その時点ではメモリー上に存在しなければならない。
従って、仮想記憶を実現するには、実際のメモリー（「物理メモリー」または「実メモリー」と言う）の内容と、ディスク上の仮想メモリーの内容を部分的に置き換えながら（スワップしながら）プログラムを実行する必要がある。

仮想記憶の手法には、「セグメント方式」と「ページング方式」とがある。
本書はWindowsの説明に特化しているので、Webから両方の説明を拾ってきた。

> ■セグメント方式
仮想記憶領域をプログラムなど意味のある単位（セグメント）に分割
して管理する方式。プログラムから見た論理的な単位のアクセスが可能
で、プログラムに合わせて動的に領域を確保できる。
>
>■ページング方式
仮想記憶領域を固定的な大きさ（ページ）に分割して管理する方式。
ページの大きさはすべて同じであるため、実記憶領域に隙間なく格納
できて使用効率が高くなり、領域管理もページ単位で簡単に行える。
>
>([仮想記憶におけるセグメント方式とページング方](https://detail.chiebukuro.yahoo.co.jp/qa/question_detail/q10158012365))

また、[Wikipedia](https://ja.wikipedia.org/wiki/%E4%BB%AE%E6%83%B3%E8%A8%98%E6%86%B6)を見る限り、Linuxではセグメント方式、Windowsではページング方式が採用されている。


> Linuxでは、`free`コマンドでメモリーの使用状況を確認できる。
以下の例では、`-h`オプションで見やすい単位に変換して表示させている。
```bash:free
$ free -h
              total        used        free      shared  buff/cache   available
Mem:           7.6G        1.9G        3.0G        333M        2.7G        5.1G
Swap:          7.8G          0B        7.8G
```
キャッシュとバッファーで2.7GBを使用しており、スワップ領域として7.8GB確保していることがわかる。
以上、本書には載っていなかったが念の為記しておく。

### メモリーを節約するプログラミング手法
メモリー不足を根本的に解決するには、実メモリーの容量を増やすか、実行するアプリケーションのサイズを小さくする工夫が必要になる。
ここでは、後者のアプリケーションのサイズを小さくする方法を2つ挙げる。

#### 1. DLLファイルで関数を共有する
DLL（Dynamic Link Library）ファイルとは、プログラムの実行時にライブラリ（関数やデータ）が動的に結合されるというもの。
注目すべきは、複数のアプリケーションがDLLファイルを共有するということ。
これによってメモリーを節約することができる。

例えば、何らかの処理昨日を持った関数`MyFunc()`を作成したとする。
そして、この関数をアプリケーションAおよびアプリケーションBから利用する。
それぞれのアプリケーションの実行ファイルの中に関数`MyFunc()`を組み込み（これを「スタティック・リンク」と呼ぶ）、2つのアプリケーションを同時に実行すると、メモリー上に同じ関数`MyFunc()`のプログラム・コードが2つ存在することになる。
これでは、メモリーの利用効率が悪くなる。（片方が無駄）

これに対して、関数`MyFunc()`をアプリケーションの実行ファイルではなく、独立したDLLファイルに組み込んでみる。
すると、、同じDLLファイルの内容が実行時に複数のアプリケーションから共有されるので、メモリー上に存在する関数`MyFunc()`のプログラム・コードは1つだけになる。
これだと、メモリーの利用効率が高まり、メモリーの節約になる。

Windowsだと、OS自体が複数のDLLファイルの集合体となっていたり、C++だとクラス・ライブラリがDLLファイルになっている。
このように多くのアプリケーションから共通して利用される機能がDLLファイルとなっている理由は、メモリーの節約のみならず、拡張性にもある。
つまり、DLLファイルを変更しても、それを参照するアプリケーションの再コンパイルが不要であるというメリットも存在する。

#### 2. _stdcall呼び出しでプログラムのサイズを小さくする
_stdcall呼び出しはC言語の話になるが、この考え方を一般化すれば他の言語にも活用できる。

C言語では、関数を呼び出した後に、スタックのクリーンアップ処理を行う命令を実行する必要がある。
スタックのクリーンアップ処理とは、関数のパラメータを引き渡すためなどに使われるメモリー上のスタックの中から不要となったデータを削除すること。
この命令は、プログラマが記述する必要はなく、プログラムのコンパイル時に、コンパイラが自動で付け加えてくれる。
コンパイラのデフォルト設定では、この処理は関数の呼び出し側に付け加えられる。

```c:call_MyFunc
// 呼び出し側
void main(int, argc, char* argn[])
{
    int i;
    i = MyFunc(123, 456);
}

// 呼び出される側
int MyFunc(int a, int b)
{
    return a + b;
}
```

例えば上の例では、関数`main()`から関数`MyFunc()`を呼び出している。
デフォルトの設定では、スタックのクリーンアップ処理が関数`main()`側に付け加えられる。

スタックのクリーンアップ処理の内容は、コンパイラが作成したマシン語の実行ファイルの内容を調べればわかるが、マシン語のままではわかりにくいので、アセンブリ言語のコードで示してみる。
最後の1行の処理がスタックのクリーンアップ処理。

```assembly:call_MyFunc
push 1C8h      ←456(=1c8h)というパラメータをスタックに格納する
push 7Bh       ←123(=7Bh)というパラメータをスタックに格納する
call @LTD+15(MyFunc)(00401014)    ←MyFunc()を呼び出す
add esp, 8     ←スタックのクリーンアップ処理を行う
```

C言語では、スタックを使って関数のパラメータを渡す。
`push`は、スタックにデータを格納する命令のこと。
1回の`push`命令で、4バイトのメモリーが使用される。
`push`命令を2回使って2つのパラメータ（456と123）をスタックに格納したので、合成8バイトのメモリーが使用された。
`call`命令で`MyFunc()`が呼び出された後は、スタックに格納されたデータが不要になる。
そこで、`add esp, 8`という命令で、スタックのデータ格納位置を表すespレジスタを8バイト戻し（8バイト上位アドレスを指すよう設定し）、データを削除する。
スタックは様々な場面で再利用されるメモリー領域なので、使い終わったら元の状態に戻す処理が必要となる。
これがスタックのクリーンアップ処理の内容である。
なお、C言語では、関数の戻り値は、スタックではなく、レジスタを使って返されるようになっている。

先程も述べたように、デフォルトの設定では、スタックのクリーンアップ処理が関数を呼び出す側（関数`main()`側）に付け加えられる。
しかし、クリーンアップ処理を、呼び出された関数側で行うことで、呼び出す側で行う場合よりもプログラム全体のサイズが小さくなる。
その際に使用するのが`_stdcall`というキーワード。
関数の前に`_stdcall`を置くことで、スタックのクリーンアップ処理を呼び出された関数の側で行うように変更できる。
先程のC言語のコードの中の`int MyFunc(int a, int b)`の部分を`int _stdcall MyFunc(int a, int b)`として再コンパイルすると、先程のアセンブリ言語のコードの最後の行の`add esp, 8`と同様の処理が、関数`MyFunc()`の内部で行われるようになる。
これによって節約できるプログラムのサイズは、わずか3バイト（`add esp, 8`は3バイトを占める）だが、何度も呼び出される関数では効果がある。
例えば1つのアプリケーションの中で3つの関数`A`, `B`, `C`をそれぞれ10回ずつ呼び出すとしたら、デフォルトなら3バイト×10回×3つ＝90バイトの処理が、3バイト×1回×3つ＝9バイトの処理となり、81バイトの節約になる。

> ここはよくわからなかったので、C言語を勉強する際にもう一度勉強したい。

### ディスクの物理構造
ディスクは、その表面を物理的にいくつかの領域に区切って使う。
その方法には、固定長の領域に区切る「セクター方式」と、可変長の領域に区切る「バリアブル方式」がある。
一般的なパソコンが使っているハードディスクでは、セクター方式が採用されている。
セクター方式では、ディスクの表面を同心円状に区切った領域を「トラック」と呼び、トラックを固定長サイズに区切った領域を「セクター」と呼ぶ。
セクターが、ディスクを物理的に読み書きする最小単位となる。
Windowsが論理的にディスクを読み書きする単位は、セクターの整数倍の「クラスタ」と呼ぶ。
異なるファイルを同一のクラスタに詰めて格納することはできない。従って、どんなに小さなファイルであっても、1クラスタの領域を占有することになる。
これはもったいないように思えるが、クラスタのサイズを小さくすると、その分ディスクのアクセス回数が増えることになり、結果としてファイルの読み書きにかかる時間が遅くなる。
ディスクの表面には、セクターの区切りを示す領域も必要なため、クラスタのサイズを小さくしすぎると、ディスク全体の記憶容量が減ってしまう。
セクターやクラスタのサイズは、処理速度と記憶容量のバランスを取って決められている。

> HDDとSSDの違いは？
気になったので調べてみると、どうやらSSDはメモリーをディスクとして使っているらしい。
なので、読み書きが早く、ディスクを回転させる必要もないので衝撃に強く、静音性も高い。
すごく納得した。
ただし、高価であること、熱に弱いこと、寿命が短いことがデメリットとして挙げられる。
ちなみにSSDはSolid State Driveの略であり、Solid State Diskではない。
ディスクではなくメモリーなのだから当たり前だ。

## 6. 自分でデータを圧縮してみよう
### 問題
本編に入る前の確認問題
1. ファイルにデータが記録される基本単位は、何でしょうか？
1. DOC，LZH，TXTの中で、圧縮ファイルの拡張子であるものは。どれでしょうか？
1. ファイルの内容を「データ×繰り返し回数」で表すことで圧縮する技法は、ランレングス法とハフマン法のどちらでしょうか？
1. パソコンでよく使われるシフトJISコードという文字コードでは、半角英数の1文字を何倍とのデータで表すでしょうか？
1. BMP（ビットマップ）形式の画像ファイルは、圧縮されているでしょうか？
1. 可逆圧縮と非可逆圧縮の違いは、何でしょうか？

答え
1. ファイルにデータが記録される基本単位は、何でしょうか？  
1バイト
1. DOC，LZH，TXTの中で、圧縮ファイルの拡張子であるものは。どれでしょうか？  
LZH
1. ファイルの内容を「データ×繰り返し回数」で表すことで圧縮する技法は、ランレングス法とハフマン法のどちらでしょうか？  
ランレングス法（AAABB→A3B2）
1. パソコンでよく使われるシフトJISコードという文字コードでは、半角英数の1文字を何倍とのデータで表すでしょうか？  
1バイト
1. BMP（ビットマップ）形式の画像ファイルは、圧縮されているでしょうか？  
圧縮されていない
1. 可逆圧縮と非可逆圧縮の違いは、何でしょうか？  
圧縮されたデータを元通りに戻せるのが可逆圧縮、戻せないのが非可逆圧縮

### ファイルの記録単位：バイト
ファイルとは、ディスクなどの記録媒体にデータを格納したもの。
ファイルに格納されるデータの単位は、バイト。
ファイルのサイズが〇〇MBなどで表現されるのは、バイトを単位にしているから。
つまり、ファイルはバイト・データの集まりということになる。
1バイト（8ビット）で表せるバイト・データは256種類あり、2進数では00000000〜11111111の範囲になる。
ファイルに格納された個々のデータが、文字を意味しているなら文章ファイルとなり、画像を意味しているなら画像ファイルとなる。
いずれにせよ、ファイルの中にはバイト・データが連続的に格納されている。

### ランレングス法
ファイルの内容を「データ×繰り返し回数」で表すことで圧縮する方法を「ランレングス法」(Run Length Encoding)と呼ぶ。
これも立派な圧縮技法のひとつであり、ファクシミリの画像圧縮などに用いられている。

しかし、実際の文章ファイルでは、同じ文字が何度も続いている部分が滅多にない。
ランレングス法は、同じデータが続いている場合が多い画像ファイルなどで高価を発揮するが、文章ファイルの圧縮には向いていない。

あと、圧縮の仕組みがシンプルなので、ランレングス法を使ったプログラムは容易に作成できる。

### ハフマン法
ハフマン法は、1952年にハフマンによって考案された。
ハフマン法を理解するためには、まずレングス法での「アルファベット1文字が1バイトのデータ」だということを忘れる。
文章ファイルは、様々な種類の文字から成り立っているが、それぞれの文字が登場する回数が異なる。
例えば一つの文章ファイルの中で「A」は100回使われているが、「＆」は3回しか使われない、など。
これをうまく利用し、「何回も使われているデータは8ビットより少ないビット数で表し、あまり現れないデータを表すには8ビットを超えても構わない」と考えるのが、ハフマン法による圧縮の概念である。

先程の「A」と「＆」の例から、メモリーの占有バイト数を比較してみる。
- 「A」と「＆」の両方を8ビットで表した場合  
100回×8ビット＋3回×8ビット＝824ビット
- 「A」を2ビット、「＆」を10ビットで表した場合  
100回×2ビット＋3回×10ビット＝230ビット

ただし、8ビットに満たないデータや、8ビットを超えるデータであっても、最終的には8ビット単位にまとめてファイルに格納されることになる。
例えば、以下のような2ビット、2ビット、5ビット、4ビット、3ビットのデータがあった場合でも、ファイルには8ビット単位で格納される。
| 01 | 11 | 10101 | 1001 | 101 |　→　| 01/11/1010 | 1/1001/101|

この処理を実現するためには、圧縮プログラムの内容が非常に複雑になるが、その見返りとして極めて高い圧縮率が得られる。

ちなみにモールス信号も同じような原理で設計されている。
単音「1」と長音「11」、無音「0」の3つで構成されており、出現頻度の高い文字ほど短い信号となっている。
つまり、文字ごとにビット長が異なり、ハフマン法と類似している。

### ハフマン符号の強み
モールス符号は一般的な文章における頻度を基に個々の文字を表す符号のデータ長を求めている。
しかし、日本語のローマ字読みの文章など特殊な文章に対しては、その符号のデータ長が最適ではなくなる。
本来はデータごとに出現頻度の高い文字を特定し、その文字に最も短いデータ長の符号を割り当てるべきだ。

ハフマン符号では、圧縮対象となるファイルごとに最適な符号体型を構築し、それを基にして圧縮を行う。
従ってファイルごとにどのデータにどの符号が割り当てられているのかが異なる。
そのため、ハフマン法で圧縮されたファイルには、符号の情報と圧縮されたデータの両方が格納される。

例えば、「A＝0」「E＝1」「B＝10」「C＝100」となったとき、圧縮後の「100」は数通りの解釈ができる。
- |100|の場合：「 C」
- |10| 0 | の場合：「BA」
- |1| 0 | 0 |の場合：「EAA」

ハフマン法では、ハフマン木を使って符号体型を構築することで、この問題を解決している。
ハフマン木を使えば、個々の文字を表すビット数が異なっていても、区切りのわかる符号を確実に作成でき、文字を区切る符号を別途作成する必要もない。

### ハフマン木の仕組み

1. データと出現頻度を並べる
1. 出現頻度の少ない方から2つを選び、枝を伸ばした冗談に数値の合計を書く。  
選択肢が複数ある場合は、どれを選んでも構わない。
1. 先程と同様の手順を繰り返す。
1. 最終的に根となる数値が1つになったら、ハフマン木の完成。  
末端の葉に向かって根の左にある枝に0を書き、値の右にある枝に1と書いていく。  
根から枝を辿って目的のおじに到達した時、通過した枝の0または1を順に上位桁から並べたものがハフマン符号となる。

詳しくはこちらを参照にされたし。
[データ圧縮の基礎『ハフマン符号化』の仕組みを見てみよう](https://michisugara.jp/archives/2013/huffman.html)

### 画像ファイルに使う非可逆圧縮
画像ファイルの使用目的は、画像データをディスプレイやプリンタに出力すること。
Windowsの標準画像データ形式であるBMP形式は、全く圧縮が行われていない。
BMPは、ディスプレイなどの出力デバイスのビットにそのままマッピングできるので、ビットマップと呼ばれている。
BMP以外のJPEGやTIFF、GIFなどのデータ形式はは何かしらのデータ圧縮が行われている。

画像ファイルは、個々の文字や数値に意味があるプログラム・ファイルやデータ・ファイルと異なり、人間の目から見て違和感がなければ、圧縮前と多少状態が変わっていても問題がない。
このように、圧縮の前後で状態が変わっている（圧縮前の状態に戻せないこと）を非可逆圧縮と言い、逆に圧縮前の状態に戻せることを「可逆圧縮」と言う。

## 7. プログラムはどんな環境で動くのか
### 問題
本編に入る前の確認問題
1. アプリケーションの動作環境は、何で示されますか？
1. AT互換機用のWindows 95は、PC-9801で動作するでしょうか？
1. MIPS版のWindows NT用アプリケーションは、x86版のWindows NTで動作するでしょうか？
1. FreeBSDが提供するPortsとは、何でしょうか？
1. Windowsが提供するMS-DOS環境のエミュレータを何と呼びますか？
1. Java仮想マシンの役割は、何でしょうか？

答え
1. アプリケーションの動作環境は、何で示されますか？  
OSとハードウェアの種類
1. AT互換機用のWindows 95は、PC-9801で動作するでしょうか？  
動作しない（[AT互換機とは？](https://detail.chiebukuro.yahoo.co.jp/qa/question_detail/q129748967)）
1. MIPS版のWindows NT用アプリケーションは、x86版のWindows NTで動作するでしょうか？  
動作しない
1. FreeBSDが提供するPortsとは、何でしょうか？  
アプリケーションをソースコードで提供し、環境に併せてコンパイルすることで実行可能にする仕組み
1. Windowsが提供するMS-DOS環境のエミュレータを何と呼びますか？  
「MS-DOSプロンプト」、または「コマンドプロンプト」
1. Java仮想マシンの役割は、何でしょうか？  
バイトコードとなったJavaアプリケーションを実行すること

### 動作環境＝OS＋ハードウェア
プログラムには、動作環境というものが存在し、OSとハードウェアが動作環境を決定している。
特定のOSのバージョン（以上）でしか動作しないアプリケーションが存在するため、動作環境にOSが含まれるのは想像しやすい。
（OSの挙動に依存したプログラムを組み込む場合など）
動作環境としてハードウェアを考える際には、マイクロプロセッサの種類が特に重要となる。
マイクロプロセッサは、コンピュータの頭脳に相当する電子部品であり、プログラムの内容を解釈・実行する。
マイクロプロセッサは、そのマイクロプロセッサ固有のマシン語（機械語）しか解釈できない。
例えば、x86、MIPS、SPARC、PowerPCなどのマイクロプロセッサの種類によってマシン語が違うし、同じx86でも486、Pentiumなどの細かい種類によって、マシン語が多少違うこともある。

マシン語になっているプログラムを、「ネイティブコード」と言う。
例えばC言語やPythonなどで作成したプログラムは、作成段階ではただのテキストファイルでしかない。
テキストファイルなら、（文字コードの問題を除いて）どのような環境でも表示・編集できる。
これを「ソースコード」と呼ぶ。
ソースコードをこコンパイルすることで、ネイティブコードが得られる。

### Windowsの歴史
コンピュータのハードウェアは、マイクロプロセッサ以外にも、プログラムの命令やデータを記録するためのメモリーや、キーボード、ディスプレイ、ハードディスクなど、様々なもので構成されており、それらをどのように制御するのかはコンピュータごとに異なる。
Windowsはこのようなハードウェア構成の違いを乗り越えることに大きく貢献したOSと言える。
（詳しくはこの記事を参照：[なぜWindowsが？いつから普及したかの理由と流れ](https://btopc.jp/select/why-did-windows-spread.html)）

Windowsの前進であるMS-DOSが使われていた時代、NECのPC-9801、富士通のFMR、東芝のDynabookなど国内には様々な機種のパソコンが存在していた。
Windwos3.0や3.1が出てきた頃、AT互換機が普及し始め、PC-9801とシェアを競っていた。
これらの機種はいずれも468やPentiumなどのx86マイクロプロセッサを搭載していたが、メモリーやI/Oアドレスの構成などが異なっていたので、MS-DOS用のアプリケーションは機種ごとに専用のものが必要だった。
（同じワープロソフト「一太郎」でもAT互換機用のものとPC-9801用のものが開発されていた）
なぜなら、アプリケーションの機能の中に、コンピュータのハードウェアを直接操作している部分があったから。
MS-DOSの機能が不十分だったこと、プログラムの実行速度を高める必要があったことなどがその理由。

このような状態は、Windowsが普及することで大きく改善された。
Windowsが動いている環境なら、同じアプリケーション（ネイティブコード）がどの機種でも動作することが保証されるようになったのだ。
Windowsは、機種ごとに異なる構成のハードウェアをアプリケーションの代わりに操作する。
つまり、Windows用のアプリケーションでは、キー入力も画面出力も、ハードウェアではなくWindowsに命令を与えることで間接的に実現する。
これによって、プログラマはメモリーやI/Oアドレスの構成の違いを意識する必要がなくなった。
ただし、Windows自体は、AT互換機用、PC-9801用など、機種ごとに専用のものが必要だった。

しかし、残念ながら、Windowsであってもマイクロプロセッサの種類の違いまでは吸収できない。
なぜなら、市販のWindowsアプリケーションは、特定のマイクロプロセッサのネイティブコードの形で提供されるから。
つまり、同じバージョンのOS（例えばWindows NT）であってもマイクロプロセッサごと（主流のx86以外にMIPS、Alphaなど）に作らなければいけないし、アプリケーション（ワープロ）なども、マイクロプロセッサの種類ごとに存在するWindowsのOSように作らなければならなかった。

流石にこれは面倒だということで、x86以外のマイクロプロセッサ用のWindows NTの開発は中止され、Windows2000以降はx86用しか開発されていない。

### APIはOSごとに違う
今度はOSの種類に目を向けてみる。
多くのハードウェアでは、インストールできるOSの出位にはいくつかの選択肢がある。
そして、アプリケーションはOSの種類ごとに専用のものを作る必要がある。
マイクロプロセッサの種類ごとにマシン語が違うように、OSの種類ごとにアプリケーションからOSへの命令の仕方が異なるから。

アプリケーションからOSへの命令の仕方を定めたものを「API（Application Programming Interface）」と呼ぶ。
OSごとにAPIが異なるため、同じアプリケーションを他のOSように作り直す場合には、アプリケーションがAPIを利用している部分を書き換える必要がある。
APIが提供しているのは、キー入力、マウス入力、画面出力、ファイル入出力などのように、周辺機器と入出力を行う機能。
何らかの数値の合計値を求めるような内部的な計算処理はAPIとは無関係なので、書き換える必要がない。

これとは逆に、あるOSが異なるハードウェアで動作している場合を考える。
APIはOSが同じなら、どのハードウェアでも基本的に同じになる。
そのため、特定のOSのAPIに合わせて作られたプログラムは、どのハードウェアでも動かすことができる。
もちろん、マイクロプロセッサの種類ごとにマシン語が異なるため、ネイティブコードまで同じというわけにはいかない。
この場合、それぞれのマイクロプロセッサに併せたネイティブコードを生成するコンパイラを使って、ソースコードをコンパイルし直す必要がある。

### ソースコードを簡単に利用できるFreeBSDのPorts
「マイクロプロセッサが違うことで同じネイティブコードを再利用できないなら、いっそのことソースコードのままプログラムを配布してしまえば良いのでは？」というのも、一つの方法であり、実際にUNIX系のSOの一部で活用されている。
UNIX系のOSであるFreeBSDには、「Ports」と呼ばれる仕組みがある。
これは、アプリケーションのソースコードを現在のハードウェアに合わせてコンパイルして、確実に実行できるネイティブコードを得るというもの。
Portsでは、もしも目的のアプリケーションのソースコードがハードディスク上にないなら、自動的にFTPを使ってインターネットに接続されたサイトからソースコードをダウンロードするようになっている。
FreeBDS用のアプリケーションのソースコードを提供してくれるサイトは世界中に数多くある。

FreeBDS用のアプリケーションのソースコードはC言語で記述されている。
FreeBDSのようなUNIX系のOSには、OSの一部として必ずCコンパイラが付属している。
このCコンパイラは、現在FreeBSDを動作させている環境にあったネイティブコードを生成してくれる。
従って、FreeBSDを使っているなら必ずPortsの恩恵が受けられる。
Portsはマイクロプロセッサを含めて、全てのハードウェアの違いを吸収するものだと言える。
Portsとはporting（移植する）という意味を持っている。
異なる動作環境用にプログラムを作り直すことを、一般的には「移植する」と言う。

### エミュレータ
移植せずに異なるOS用のアプリケーションを動作させる方法がある。
それが、エミュレータと呼ばれるソフトウェアを利用する方法。
身近なところでは、Windowsに標準装備されている「MS-DOSプロンプト（コマンドプロンプト）」と呼ばれるもの。
これは、WindowsというOS上で、MS-DOS用アプリケーションを動作させるエミュレータのこと。

MS-DOSプロンプトは、MS-DOS用アプリケーションに対して、あたかもMS-DOSが動作している環境のように振る舞う。
アプリケーションがMS-DOSへの命令を実行すると、MS-DOSと同じ処理を行い、同じ応答をする。
さらにアプリケーションがハードウェアを直接操作しようとしたら、Windowsが一旦命令を受け取ってから、実際のハードウェアを操作する。

また、マイクロプロセッサの垣根を超えるようなエミュレータも存在している。

> 現在ではBoot CampやVMWareなど、MacでWindowsを、WindowsでMacを起動する仮想マシンや、AWSなどの仮想サーバー、OSとディスクの情報を保存する仮想イメージ、その状態を再現するスクリプトを保存するサービス（Docker）など、更に多くの選択肢が存在している。
本書は2000年頃の出版物で流石に情報が古すぎるので、一応補足。

### Java仮想マシン
エミュレータは、ある環境を別の環境上に実現するものだが、エミュレータとは異なる手法で、特定のハードウェアやOSとは結びついていないプログラムの実行環境を提供するものもある。
それが、「Java」である。
Javaには、2つの側面がある。
1つはプログラミング言語としてのJavaであり、もう1つは実行環境としてのJavaである。
Javaは、他のプログラミング言語と同様に、Javaの文法で記述されたソースコードをコンパイルしたものを実行する。
ただし、コンパイル後に生成されるのは、特定のマイクロプロセッサ用のネイティブコードではなく、「バイトコード」と呼ばれるものである。
バイトコードの実行環境を「Java仮想マシン（JavaVM）」と呼ぶ。
Java仮想マシンは、Javaバイトコードを逐次ネイティブコードに変換しながら実行する。

1. Javaの文法でのソースコードの記述
1. コンパイラによるソースコードからバイトコードへのコンパイル
1. Java仮想マシンにバイトコードを読み込む
1. バイトコードの実行時に逐次ネイティブコードに変換し、マイクロプロセッサに処理させる

という、一見すると回りくどい手法に思えるが、同じバイトコードを異なる環境で動作させるためにこのような手法を取っている。
様々な種類のOSやハードウェアの機種に合わせてJava仮想マシンを作成しておけば、同じバイトコードのアプリケーションが様々な環境で動作する。

OSから見れば、Java仮想マシンは一種のアプリケーションだが、Javaアプリケーションから見ればJava仮想マシンは動作環境そのものになる。

Java仮想マシンにも課題は存在する。
一つは、異なるJava仮想マシンの間で完全には互換性が取れていないこと。
どのJava仮想マシンでも、あらゆるバイトコードを動作可能にさせるのはなかなか難しい。
さらに、特定のハードウェアにしかない機能を使うような場合、他のJava仮想マシンでは動作しなかったり、逆にそのような機能が使えないように制限されていたりすることがある。
もう一つは、実行速度の問題。
実行時に毎回バイトコードをネイティブコードに変換するというJavaの仕組みは、実行速度を遅くする要因となっている。
最近では、一度変換↓ネイティブコードを保存しておき、2度目以降は直接ネイティブコードを利用する技術や、バイトコードの中で処理時間が多くかかる部分の最適化などの高速化技術が進み、プログラムの実行速度が遅いという問題は次第に解決に向かっている。

### BIOSとブートストラップ
非常に低レベル（ハードウェアに近い部分）の話を最後にしておく。
プログラムの動作環境には「BIOS（Basic Input/Output System）」というものがある。
BIOSはROMなどに記録され、コンピュータ本体に内蔵されているプログラムである。
BIOSはキーボード、ディスク、グラフィックボードなどの制御プログラムの他、ブートストラップローダーという機能を持っている。

コンピュータの電源を入れると、マイクロプロセッサ、メモリー、ディスクの診断を行った後、ブートストラップローダーが実行される。
ブートストラップローダーの役割は、ハードディスクなどに記録されたOSをメモリーにロードして実行すること。
アプリケーションを起動するのはOSの役割だが、OSは自分自身を起動できない。

ブートストラップ（boot strap）とは、ブーツの上部に付いている「つまみ革」という意味。
BIOSという小さなプログラム（つまみ革）が、OSという大きなプログラム（ブーツ）を引き上げる（起動する）ことから、この名前が付けられたと言われている。
OSが起動しているなら、プログラマがBIOSのことを意識する必要はないが、それが存在しているということは覚えておくべき。

## 8. ソース・ファイルから実行ファイルができるまで
### 問題
本編に入る前の確認問題
1. マイクロプロセッサが解釈・実行できる形式のプログラムを何と呼びますか？
1. 複数のオブジェクト・ファイルを結合してEXEファイルを生成するツールを何と呼びますか？
1. 全てのプログラムの先頭に共通して結合されるオブジェクトファイルを何と呼びますか？
1. 複数のオブジェクトファイルをまとめて収録したファイルを何と呼びますか？
1. DLLファイルに記録された関数の呼び出し情報だけを持つファイルを何と呼びますか？
1. プログラムの実行時に、配列やオブジェクトのために動的に確保されるメモリー領域を何と呼びますか？

答え
1. マイクロプロセッサが解釈・実行できる形式のプログラムを何と呼びますか？  
ネイティブコード
1. 複数のオブジェクト・ファイルを結合してEXEファイルを生成するツールを何と呼びますか？  
リンカー
1. 全てのプログラムの先頭に共通して結合されるオブジェクトファイルを何と呼びますか？  
スタートアップ
1. 複数のオブジェクトファイルをまとめて収録したファイルを何と呼びますか？  
ライブラリファイル
1. DLLファイルに記録された関数の呼び出し情報だけを持つファイルを何と呼びますか？  
インポートライブラリ
1. プログラムの実行時に、配列やオブジェクトのために動的に確保されるメモリー領域を何と呼びますか？  
ヒープ

### コンピュータはネイティブコードしか実行できない
以下のコードでは123と456の加算演算の結果である579がメッセージボックスに出力される。

```c:sum_123_456
# include <windows.h>
# include <stdid.h>

// メッセージボックスのタイトル
char* Title  = "加算プログラム" ;

// 2つの数値を加算する関数
int AddNum(int a, int b)
{
    return a + b;
}

// メインルーチンとなる関数
int WINAPI WinMain(HINSTANCE hInst, HINSTANCE hDummy,
LPSTR lpszArgs, int iMode)
{
    int i;
    char s[80];

    i = AddNum(123, 456);
    sprintf(s, "123 + 456 = %d", i);
    MessageBox(NULL, s, Title, MB_OK);

    return 0;
}
```

このように何らかのプログラム言語で記述されたプログラムのことを「ソースコード」と呼び、ソースコードをファイルとして保存したものを「ソースファイル」と言う。
ソースファイルは単なるテキストファイルなので、メモ帳などのテキストエディタで作成することができる。

ソースコードをそのまま実行することはできない。
コンピュータのマイクロプロセッサが解釈・実行できるのは、ソースコードではなくネイティブコード（マシン語）のプログラムだけだから。
そのため、ソースコードをネイティブコードに変換する必要がある。
ちなみに、WindowsのEXEファイルはすでにネイティブコード変換されたものである。

ファイルの内容を1バイトずつ16進数で表示することをダンプという。
EXEファイルをメモ帳で開いて中の情報を見ようとしても、文字化けしてしまい理解できなくなってしまう。
しかし、Windowsに付属するdebug.exeなどを使うとファイルをダンプし、ネイティブコードを見ることができる。

### コンパイラ
C言語などの高水準言語で記述されたソースコードをネイティブコードに翻訳する機能を持ったプログラムを「コンパイラ」と呼ぶ。
コンパイラは、ソースコードを記述するプログラミング言語の種類に応じて、専用のものが必要になってくる。
コンパイラは、ソースコードの内容を読み込み、それをネイティブコードに翻訳してくれる。
コンパイラの中に、ソースコードとネイティブコードの対応表があるようなイメージ。
ただし、実際には対応表だけではネイティブコードを生成できない。
読み込んだソースコードの字句解析、構文解析、意味解析などを経て、ネイティブコードを生成するコンパイラがほとんど。

マイクロプロセッサの種類が異なればネイティブコードの種類も異なる。
そのため、コンパイラはプログラミング言語の種類だけではなく、マイクロプロセッサの種類に応じて専用のものが必要となってくる。

また、コンパイラ自体もプログラムの一種なので、動作環境が存在する。
つまり、OSごとに使用されるコンパイラが異なる。

さらに、動作環境で使われているマイクロプロセッサとは異なるマイクロプロセッサ用のネイティブコードを生成する「クロスコンパイラ」というものも存在する。

### コンパイル後の挙動
ソースコードの翻訳結果としてコンパイラが生成するのは、ネイティブコードのファイルだが、そのままだと実行できない。
実行可能なEXEファイルを得るためには、「リンク」という処理が必要になる。

例えばhoge.cというファイルをコンパイルすると、hoge.objというオブジェクトファイルが生成される。
オブジェクトファイルの内容はネイティブコードになっているが、まだ未完成なので実行できない。

先程のコードを再度掲載する。

```c:sum_123_456
# include <windows.h>
# include <stdid.h>

// メッセージボックスのタイトル
char* Title  = "加算プログラム" ;

// 2つの数値を加算する関数
int AddNum(int a, int b)
{
    return a + b;
}

// メインルーチンとなる関数
int WINAPI WinMain(HINSTANCE hInst, HINSTANCE hDummy,
LPSTR lpszArgs, int iMode)
{
    int i;
    char s[80];

    i = AddNum(123, 456);
    sprintf(s, "123 + 456 = %d", i);
    MessageBox(NULL, s, Title, MB_OK);

    return 0;
}
```

上のコードで使われている関数のうち`AddNum()`, `WinMain()`は自前で作成したもので、その処理内容がソースコードに記載されているが、`sprintf()`と`MessageBox()`という関数は処理内容が記載されていない。
そのため、`sprintf()`と`MessageBox()`の処理内容が記載されたコンパイル済みのオブジェクトファイルとhoge.objを結合しなければ、処理が揃わず、EXEファイルが完成しない。

複数のオブジェクトファイルを結合して1つのEXEファイルを生成する処理が「リンク」であり、リンクを行うプログラムのことを「リンカー」と呼ぶ。

オブジェクトファイルという言葉は、コンパイラによって生成されるファイルを指す一般用語だが、オブジェクトファイルの拡張子とリンカーによって生成される実行可能ファイルの拡張子がそれぞれOBJ, EXEとなるのはWindowsのみ。

### スタートアップ
全てのプログラムの先頭に結合する共通的な処理が記述されたオブジェクトファイルのことを「スタートアップ」と呼ぶ。
たとえ他のオブジェクトファイルに依存する関数を呼び出していないプログラムでも、必ずリンクを行ってスタートアップと結合する必要がある。

import32.libなどのファイルをライブラリファイルと呼ぶ。
ライブラリファイルは、複数のオブジェクトファイルをまとめて記録したもの。
リンカーにライブラリファイルを指定すると、その中から必要なオブジェクトファイルだけを抽出し、それを他のオブジェクトファイルと結合してEXEファイルを生成してくれる。

先程の`sprintf()`などの関数は、ソースコードではなく、ライブラリファイルの形でコンパイラと一緒に提供される。
このような関数のことを「標準関数」と呼ぶ。
ライブラリファイルを使うのは、ソースコードをコンパイルする手間を省くためと、リンカーのパラメータに大量のオブジェクトファイルを指定する手間を省くという2つの理由から。
もしもライブラリファイルを使わない場合、数百個の標準関数を呼び出しているプログラムをリンクする時に、それらのソースコード全てをコンパイルし、リンカーのコマンドラインにオブジェクトファイル全てを指定する必要があり、非常に面倒。

### DLLファイルとインポートライブラリ
前の章で述べたように、OSはアプリケーションから利用できる様々な機能を関数セットの形で提供している。
このような関数のことを「API（Application Programming Interface）」と呼ぶ。
例えば、先程のソースコードの中の`MessageBox()`はC言語の使用として定められている標準関数ではなく、Windowsが提供するAPIの一種で、メッセージボックスを表示する機能を提供している。

Windowsでは、APIのオブジェクトファイルが、通常のライブラリファイルではなく、「DLLファイル」と呼ばれる特殊なライブラリファイルに記録されている。
DLL（ダイナミックリンクライブラリ）ファイルは、プログラムの実行時にEXEファイルに結合されるもの。

例えばWindowsのAPIである`MessageBox()`関数のオブジェクトファイルはimport32.libというライブラリファイルに記録されているが、import32.libの中には、`MessageBox()`がuser32.dllというDLLファイルの中になるという情報と、DLLファイルが格納されているフォルダの情報だけが記録されており、`MessageBox()`のオブジェクトファイルの実体は存在しない。
このimport32.libのようなファイルを「インポートライブラリ」と呼ぶ。

それに対して、オブジェクトファイルの実体を記録し、EXEファイルに直接結合してしまう形式のライブラリファイルのことを「スタティックリンクライブラリ」と呼ぶ。
`sprintf()`のオブジェクトファイルが記録されたcw32.libはスタティックリンクライブラリである。
`sprintf()`は、数値を書式付きで文字列に変換する機能を提供する。

> Linuxではどうなの？と思ったので参考できそうなサイトを記しておく。
- [プログラマーでない人のための「共有ライブラリ」講座](https://www.glamenv-septzen.net/nifty/others/computer/linux_ldd01.html)
- [Linux - Library](https://www.infraeye.com/study/linuxz6.html)
- [Linux共有ライブラリの簡単なまとめ](https://www.wagavulin.jp/entry/20091026/1256577635)

### 実行可能ファイルの実行に必要なこととは？
次にEXEファイルが実行される仕組み説明する。
EXEファイルは単独のファイルとしてハードディスクに記録されている。
エクスプローラでEXEファイルをダブルクリックすれば、EXEファイルの内容がメモリーにロードされて実行される。

ここで一つ疑問が生まれる。
ネイティブコードでは、プログラムの中に記述された関数を参照する際に、データが格納されたメモリーアドレスを参照する命令を実行する。関数の呼び出しとは、関数の処理内容が格納されたメモリーアドレスの先頭にプログラムの流れをジャンプさせること。
EXEファイルは、ネイティブコードのプログラムとして完成されたものだが、変数や関数が物理的に何番地のメモリーアドレスに格納されているのかまでは決定していない。
WindowsのようにOSが複数の実行可能プログラムをロードできる環境では、プログラム内の変数や関数が何番地のメモリーアドレスに配置されるかは、プログラムを実行する度に異なる。
では、EXEファイルの中で、変数や関数のメモリーアドレスの値はどうなっているのか？

答えはシンプルで、EXEファイルの中では、変数や関数に仮のメモリーアドレスが与えられている。
プログラムの実行時に、仮のメモリーアドレスが実際のメモリーアドレスに変換される。
リンカーはEXEファイルの先頭に、メモリーアドレスの変換が必要な部分を示す情報を付加する。
この情報のことを「再配置情報」と呼ぶ。
EXEファイルの持つ再配置情報は、変数や関数の相対アドレスになっている。
相対アドレスとは、基点となるアドレスからのオフセット、すなわち相対距離を示すもの。
相対アドレスを利用するために、たとえソースコード内で変数と関数が様々な位置にバラバラに記述されていたとしても、リンク後のEXEファイルの中では、変数と関数がそれぞれグループにまとめられるようになっている。
従って、個々の変数のメモリーアドレスは、変数のグループの先頭を基点としたオフセットで表すことができ、個々の変数のメモリーアドレスは関数のグループの先頭を基点としたオフセットで表すことができる。
それぞれのグループの基点のメモリーアドレスがプログラムの実行時に決定されるという仕組みになっている。

### スタックとヒープ
EXEファイルの内容は、変数のグループと関数のグループに分けられていることがわかった。
ただし、プログラムがロードされたメモリー領域には、これら以外にも2つのグループが作られる。
それが「スタック」と「ヒープ」。
スタックは、関数の内部で一時的に使用される変数（内部変数）や、関数を呼び出すときのパラメータをっ格納するためのメモリ＝領域。
ヒープは、プログラムの実行時に動的に作成される配列やオブジェクトを格納するためのメモリー領域。
ここでのオブジェクトとは、オブジェクトファイルのオブジェクトではく、オブジェクト指向プログラミングのオブジェクトのこと。

EXEファイルの中にスタックやヒープのためのグループが存在するわけではない。
EXEファイルをメモリーにロードした時点で、スタックとヒープのためのメモリー領域が確保される。
従って、メモリー上のプログラムは、変数のための領域、関数のための領域、スタックのための領域、及びヒープのための領域という4つのグループから構成されていることになる。
もちろんメモリー上には、OSをロードする領域など、他の領域も存在している。

スタックやヒープは、内部変数やオブジェクトが必要になった時に、それらの領域が確保されているという点で似ているが、違いもある。
スタックにデータの格納と破棄（クリーンアップ処理）を行うコードは、コンパイラによって自動的に生成されるので、プログラマが意識する必要はない。
スタックを使うデータのためのメモリー領域は、1つの関数が呼び出されると確保され、関数の処理が終わると自動的に開放される。
それに対して、ヒープのためのメモリー領域は、プログラマが記述したプログラムによって明示的に確保と解法を行う。

C言語では`malloc()`関数でヒープのメモリー領域を確保し、`free()`関数で解放する。C++では`new`演算子でメモリーを確保し、`delete`演算子で解放する。
C言語やC++では、プログラムで明示的にヒープを解放しないと、プログラムの終了後もメモリー領域が確保されたままになる。
この現象は、「メモリーリーク（memory leak）」と呼ばれ、C言語やC++のプログラマの間で恐れられているバグの一種。
メモリーリークが蓄積されると、いずれメモリー不足に陥り、コンピュータがダウンしてしまうから。

### Q&A
Q：コンパイラとインタプリタの違いは？
A：コンパイラは、実行前にソースコードをネイティブコードに一括して翻訳する。
　　インタプリタは実行時にソースコードの内容を1行ずつネイティブコードに逐次変換する。

Q：分割コンパイルとは？
A：プログラムの全体を複数のソースコードに分けて記述し、それらを別々にコンパイルして、最終的に1つのEXEファイルにリンクすること。そうすることで、一つ一つのソースコードが短くなるので、プログラムの管理がしやくすなる。

Q：DLLファイルを使うメリットは？
A：DLLファイルの中にある関数は、複数のプログラムから共有される。これにより、メモリーとディスクが節約できる。関数の内容を修正しても、それを利用しているプログラムをリンクし直さなくてよいというメリットもある（5章参照）

Q：インポートライブラリをリンクしないとDLLファイルの中にある関数を呼び出せないの？
A：`LoadLibrary()`, `GetProcAddress()`というAPIを使えば可能だが、DLLファイルを使った方が簡単。

Q：オーバーレイリンクとは？
A：同時に実行されることのない関数を、同じアドレスにダイナミックにロードして実行するもの。「オーバーレイリンカー」という特殊なリンカーを使うことで実現できる。コンピュータに搭載されたメモリーの容量が少なかったMS-DOS時代に使われていた。

Q：「ガーベッジコレクション」とは？
A：Javaなどの言語では、ガーベッジコレクションという機能を言語処理系が備えていて、プログラマがヒープのためのメモリー解放を行わなくても、使われなくなったメモリー領域が自動的に解法されるようになっている。ガーベッジコレクションは、メモリーリークを防ぐ仕組みの一つ。

### なぜPythonはコンパイラが必要ないのか？
気になったので、調べてみる。
> 【スクリプト言語】 
低速で動作する代わりに短い記述で処理を実現。 
ソースコードを逐次機械語に翻訳しながら実行する。 
例：Python、Perl、PHP、Ruby、JavaScript
>
>([コンパイル言語とスクリプト言語の違い](https://teratail.com/questions/51394))

## 9. OSとアプリケーションの関係
### 問題
本編に入る前の確認問題
1. モニタープログラムの機能とは、何ですか？
1. OSの上で動作するプログラムのことを何と呼びますか？
1. Windows 2000は、何ビットOSですか？
1. GUIとは、何の略称ですか？
1. WYSIWYGとは、何ですか？

答え
1. モニタープログラムの機能とは、何ですか？  
プログラムのロードと実行（OSの原型であると言える）
1. OSの上で動作するプログラムのことを何と呼びますか？  
アプリケーション（または応用プログラム）
1. Windows 2000は、何ビットOSですか？  
32ビットOS
1. GUIとは、何の略称ですか？  
Graphical User Interface
1. WYSIWYGとは、何ですか？  
What You See Is What You Get.（ディスプレイに表示されたものを、そのままプリンタで印刷できること）

### OSの歴史
まだOSが存在していなかった大昔のコンピュータでは、全くプログラムのない状態から、プログラマがあらゆる処理を行うプログラムを全て作成していた。
マシン語でプログラムを記述し、それをスイッチを使って入力していた。
これでは、あまりにも面倒だということで、OSの原型が表れた。
プログラムをロードする機能と実行する機能だけを備える「モニタープログラム」が開発された。
あらかじめモニタープログラムを起動させておけば、必要に応じて様々なプログラムをメモリーにロードして実行できる。
全くプログラムのない状態から開発することと比べれば、だいぶ楽になっている。

時代は進み、モニタープログラムを利用するプログラムを何種類か作成しているうちに、多くのプログラムに共通した部分があることがわかってきた。
例えば、キーボードから文字データを入力したり、ディスプレイに文字データを出力する部分など。
これらのプログラムコードは、プログラムの用途が異なっていても共通している。
新しいプログラムを作成する度に、同じプログラムコードを記述するのは無駄なこと。
そこで、基本的な入出力を行う部分的なプログラムがモニタープログラムに追加された。
これが初期のOSの誕生である。

更に時代は進み、プログラマの便宜を図るためのハードウェア制御プログラム、言語プロセッサ（プログラミング言語だと考えて良い）および様々なユーティリティなどの機能が追加され、結果として現在のOSとほぼ同様の形態となった。
これによって、OSとアプリケーションプログラムの役割も明確に分けられた。
OSとは、単独のプログラムではなく、複数のプログラムの集合体である。

### OSの存在を意識する
アプリケーションを作成するプログラマに意識してほしいのは、ハードウェアではなく、OSの機能を利用するアプリケーションを作成しているということ。プログラマには、ハードウェアの基本的な知識を持つことも必要だが、OSが存在する以上、ハードウェを直接制御するプログラムコードを記述することは滅多にない。
アプリケーションを作成するプログラマは、ハードウェアから隔離されていることになる。
難しいハードウェアのことを意識しないでもアプリケーションを作成できるので、そのありがたみを享受すべき。

ただし、一人前のプログラマになるためには、基本t系なハードウェアのことがわかっていて、それがOSによって抽象化されているため、効率的にプログラミングできるという事実を知っておくべき。
そうでないと、何らかのトラブルに遭遇した時に、それを解決する術が見つからなくなる。
楽をするだけではダメで、なぜ楽ができるかを知ってから楽をするべき。

OSが動作しているアプリケーションは、ハードウェアを直接制御せずに、OSを介して間接的にハードウェアを制御している。
アプリケーションから命令を受けたOSが、その命令を解釈し、各ハードウェアの要素にアクセスし、直接制御をしている。

### システムコールと高水準言語の移植性
OSのハードウェア制御機能は、小さな関数の集合体として提供されているのが一般的。
これらの関数、および関数を呼び出す行為のことを「システムコール」と呼ぶ。
アプリケーションがOSの機能を呼び出すという意味。

C言語などの高水準言語では独自の関数名を使い、それがコンパイルされる時に該当するシステムコールに変換されるという仕組みになっている。すなわち、高水準言語で記述されたアプリケーションをコンパイルすると、システムコールを実行するネイティブコードになる。ただし、周辺装置との入出力を伴わず、単に数値を計算するだけの関数なら、システムコールを実行するものとはならない。それは、数値を計算するネイティブコードになる。

高水準言語の中には、直接システムコールを呼び出すことが可能な言語も存在する。
しかし、そのようなスタイルで作成されたアプリケーションは移植性の悪いものとなってしまう。
（行儀の悪いアプリケーションと俗称されることもある）

### OSと高水準言語によるハードウェアの抽象化
OSが提供するシステムコールによって、プログラマはハードウェアを直接制御するプログラムコードを記述する必要がない。
さらに、高水準言語を使うことによってハードウェアの種類の違いを意識する必要もなくなる。
OSと高水準言語によって、ハードウェアを完全に抽象化できるという素晴らしい状況が作り出されている。

例えば、MyFile.txtというファイルに「こんにちは」という文字列を書き込むプログラム（アプリケーション）を考える。
ファイルとは、OSがディスク媒体の領域を抽象化したものにほかならない。
実際には、5章で出てきたように、ハードウェアとしてのディスク媒体は、バウムクーヘンを切り分けたようなセクターに区切られ、セクター単位でデータを読み書きしている
ハードウェアを直接操作するなら、ディスク用のI/Oにセクターの位置を指定してデータを読み書きすることになるが、実際のプログラムコードにはそのような記述は不要である。
つまり、ディスク媒体の読み書きにファイルという概念を採用し、ファイルを開いて、書き込んで、最後に閉じるという手順を抽象化している。

### WindowsというOSの特徴
情報が古いし、Windowsという普段使わないOSの話なので割愛する。
一応挙げられている特徴だけを記載しておく。

1. 32ビットOSである
1. APIやCOMによってシステムコールを提供する
1. GUIを採用したユーザーインターフェースを提供する
1. WYSIWYGによるプリンタ出力ができる
1. マルチタスク機能を提供する
1. ネットワーク機能やデータベース機能を提供する
1. プラグ＆プレイによるデバイスドライバの自動設定ができる

### シェルスクリプト
シェルスクリプトとは、OSのコマンドを、実行する順番に記述したテキストファイルのこと。
環境変数などと呼ばれる変数を持つこともでき、分岐やループも記述できるので、ちょっとしたプログラミングが可能。
シェルスクリプトという言葉は、主にUNIXの世界で使われる。
メインフレームの世界では、シェルスクリプトのことをジョブ制御言語（JCL = Job Control Language）と呼び、MS-DOSではバッチファイルと呼ぶ。

## 10. アセンブリ言語からプログラムの本当の姿を知る
### 問題
本編に入る前の確認問題
1. ネイティブコードの命令に、英語に似たニックネームを付けたものを何と呼びますか？
1. アセンブリ言語において、アセンブラ自体に対する命令を何と呼びますか？
1. ネイティブコードをアセンブリ言語のソースコードに逆変換することを何と呼びますか？
1. アセンブリ言語のソースファイルの拡張子は、一般的に何でしょうか？
1. プログラムにおけるセグメントとは何ですか？
1. C言語の関数のパラメータは、何というメモリー領域を介して渡されますか？

答え
1. ネイティブコードの命令に、英語に似たニックネームを付けたものを何と呼びますか？  
ニーモック
1. アセンブリ言語において、アセンブラ自体に対する命令を何と呼びますか？  
疑似命令
1. ネイティブコードをアセンブリ言語のソースコードに逆変換することを何と呼びますか？  
逆アセンブル
1. アセンブリ言語のソースファイルの拡張子は、一般的に何でしょうか？  
.asm
1. プログラムにおけるセグメントとは何ですか？  
プログラムを構成する命令やデータをまとめたグループのこと
1. C言語の関数のパラメータは、何というメモリー領域を介して渡されますか？  
スタック

### アセンブリ言語とは
これまで説明してきた通り、コンピュータのマイクロプロセッサが解釈・実行できるのはネイティブコード（マシン語）のプログラムのみ。
C言語などのプログラム言語で記述されたソースコードは、それぞれのプログラミング言語用のコンパイラでコンパイルすることで、ネイティブコードに変換される。
ネイティブコードの内容を調べれば、プログラムが最終的にどのような姿になって実行されているのかがわかる。
しかし、ネイティブコードはただの数値の羅列であり、直接この数値を使ってプログラムを作成したのでは、わかりづらい。
そこで、個々のネイティブコードに、機能を表す英語のニックネームをつけようという手法が考案された。
例えば、加算を行うネイティブコードには`add`、比較を行うネイティブコードには`cmp`という形で。
このようなニックネームのことを「ニーモニック」と呼び、ニーモニックを使うプログラム言語のことを「アセンブリ言語」と呼ぶ。
アセンブリ言語で記述されたソースコードを見るのは、ネイティブコードのソースコードを見るのと同レベルなので、アセンブリのソースコードを見ることで、本当のプログラムの姿を知ることができる。

アセンブリ言語で記述されたソースコードであっても、最終的にはネイティブコードに変換しなければ実行できない。
そのための変換プログラムのことを「アセンブラ」と呼ぶ。
ただし、アセンブリ言語やアセンブリ言語で書かれたプログラムのことをアセンブラと呼ぶ場合もある。
ソースコードをネイティブコードに変換するという機能に置いて、アセンブラとコンパイラは同様のものだと言える。

アセンブリ言語で記述されたソースコードとネイティブコードは1対1に対応する。
（ただし、アセンブリ言語には、ネイティブコードに変換されない疑似命令やマクロ命令もある）
したがって、ネイティブコードからアセンブリ言語のs−スコードに逆変換することも可能。
このような機能を持った逆変換プログラムのことを「逆アセンブラ」と呼ぶ。

C言語やBASICなど、異なるプログラミング言語を使って記述されたソースコードであっても、コンパイル語には特定のマイクロプロセッサ用のネイティブコードになる。それを逆アセンブルすれば、アセンブリ言語のソースコードが得られ、その内容を調べることができる。
ただし、ネイティブコードをC言語などのソースコードに逆コンパイルすることは、逆アセンブルと比べて困難になる。
C言語などのソースコードはネイティブコードと1対1対応していないので、必ず元通りのソースコードが得られるわけではないから。

ネイティブコードを逆アセンブルする方法以外にも、コンパイラでアセンブリ言語のソースコードを出力することで、アセンブリ言語のソースコードが得られる。

### 疑似命令
アセンブリ言語のソースコードを初めて見ると面を食らうが、実際にはシンプルなもので、C言語などより簡単だと言っても過言ではない。
アセンブリ言語のソースコードを読み込ますためのポイントを説明しておく。

アセンブリ言語のソースコードは、ネイティブコードに変換される命令と、アセンブラ自体に対する命令である「疑似命令」から構成されている。
疑似命令は、プログラムの構造やアセンブルの方法をアセンブラに指示するもので、アセンブルしてもネイティブコードに変換されない。

```assembly:segment
_TEXT segment dword public use32 'CODE'
_TEST ends
_DATA segment dword public use32 'DATA'
_DATA ends
_BSS segment dword public use32 'BSS'
_BSS ends
DGROUPgroup __BSS,_DATA

_TEXT segment dword public use32 'CODE'

_AddNum        proc    near 
_AddNum        endp

_MyFunc        proc    near
_MyFunc        endp

_TEXT ends
    end
```

疑似命令`segment`と`ends`で囲まれた部分は、プログラムを構成する命令やデータをまとめたグループに名前を付けたもので、「セグメント」と呼ばれる。
一般用語としてのセグメントとは「領域」のことだが、プログラムにおけるセグメントとは、命令やデータなどを分類したプログラムの領域という意味。
1つのプログラムは、複数のセグメントから構成される。

上のソースコードの先頭で、`_TEST`, `_DATA`, `_BSS`という名前の3つのセグメントが定義されている。
`_TEST`は命令のセグメント、`_DATA`は初期化された（初期値を持つ）データのセグメント、 `_BSS`は初期化されていないデータのセグメントとなる。
このようなセグメントの名前やセグメントの分け方はコンパイラごとによって異なる。（この例はBorland C++の仕様）

それぞれの`segment`と`ends`の間には、何も記述されていないが、それでもプログラムを構成するセグメントの配置順序を`_TEST`, `_DATA`, `_BSS`にする効果がある。

`group`という疑似命令は、`_DATA`, `_BSS`という2つのセグメントを`DGROUP`という名前の大きなグループとしてまとめることを意味している。
なお、8章で示した通り、スタックとヒープのためのメモリー領域はプログラムの実行時に作成される。

`_AddNum`と`_MyFunc`を囲んでいる`_TEXT segment`と`_TEXT ends`は、`_AddNum`と`_MyFunc`が`_TEXT`というセグメントに属することを示している。
これによって、ソースコード上で命令とデータを混在させて記述しても、コンパイル後やアセンブル後には、整然とセグメントに分けられたネイティブコードになる。

`_AddNum proc`と`_AddNum endp`で囲まれた部分、`_MyFunc proc` と`_MyFunc endp`で囲まれた部分は、それぞれ`AddNum`関数と`MyFunc`関数の範囲を示している。
コンパイル後の関数名の前にアンダースコアがつくのもBorland C++の仕様。
疑似命令`proc`と`endp`で囲まれた部分は、プロシジャ（procedure）の範囲を意味する。
アセンブル言語では関数のことをプロシジャと呼ぶ。

末尾の`end`疑似命令は、ソースコードの終わりを意味している。

### アセンブリ言語の構文は「オペコード＋オペランド」
アセンブリ言語では、1行でマイクロプロセッサに対する1つの命令を表す。
アセンブリ言語の命令は、「オペコード＋オペランド」という構文になっている。
（ただし、オペコードだけでオペランドがない命令もある）
for文やif文は存在しない。
ソースコードが単純な構文の羅列となるので、アセンブリ言語はC言語よりも簡単だと言える。

オペコードは命令の動作を表し、オペランドは命令の対象となるものを表す。
オペコードとオペランドを並べる構文は、英語の命令文と同じ。
オペランドが複数ある場合はカンマで区切る。
どのようなオペコードが使えるのかは、マイクロプロセッサの種類によって決まっている。

ネイティブコードは、メモリーにロードされてから実行される。
メモリーの中には、ネイティブコードとなった命令とデータが格納される。
マイクロプロセサは、メモリーから命令やデータを読み出し、それをマイクロプロセッサの内部にあるレジスタに格納して処理を行う。
1章で見た通り、レジスタはプロセッサの中にあるメモリーのようなもので、単に命令やデータを記録するだけでなく、演算を行う機能もある。
レジスタの名前は、アセンブリ言語のソースコードでオペランドに指定する。
なお、マイクロプロセッサの内部には、プログラムが直接操作できないレジスタもある。
計算結果の正負や桁上がりの状態を示すフラグレジスタや、OS専用のレジスタなどは、我々が作成するプログラムから直接操作できない。

> 本書での例が32bitのOSのアセンブリ言語で古いので、こちらのページを参考にしたほうが良いかもしれない。
[アセンブラに手を出してみる](https://qiita.com/edo_m18/items/83c63cd69f119d0b9831)
>
> 以下アセンブリ言語の入門のような内容になり、実際のアセンブリ言語のコードも記載されているが、この辺りはアセンブリ言語を学ぼうとした段階で、それぞれの環境にあったアセンブリ言語の情報を学んだほうが良いし、この本よりもアセンブリ言語そのものを扱っている入門書に説明を預けたほうが良いので、概要だけを記していく。



### スタックへのプッシュとポップ
プログラムの実行時には、スタックと呼ばれるデータ領域がメモリー上に確保される。
スタック（stack）とは、「干し草を詰んだ山」という意味で、データはメモリーの下（値の大きいアドレス）から上（値の小さいアドレス）に向かって積み上げるように格納され、上から下に向かって山を低くしていくように読み出される。

スタックは一時的に使われるデータが格納される領域であり、`push`命令と`pop`命令でデータの格納と読み出しを行うのが特徴。
スタックにデータを格納することを「プッシュする」と呼び、スタックからデータを読み出すことを「ポップする」と呼ぶ。
例えば32ビットのx86系のマイクロプロセッサでは、1回のプッシュ／ポップで32ビット（4バイト）のデータを取り扱うことが可能。
4章では、スタックを実現するためにプッシュとポップを行う関数を作ると説明したが、アセンブリ言語を使えば、マイクロプロセッサ自体の持つ`push`命令と`pop`命令が使える。

### 外部変数と内部変数
C言語では、関数の外側で定義された変数を「外部変数」（or グローバル変数）と呼び、関数の内側で宣言された変数を「内部変数」（or ローカル変数）と呼ぶ。
外部変数はソースコードのあらゆる部分から参照できるが、内部変数は宣言された関数の中からしか参照できない。
外部変数のためのメモリー領域は常に確保されているから、プログラムのはじめから終わりまで、あらゆる場所から外部変数を参照できる。
それに対して内部変数は、レジスタやスタックを使って一時的に確保されるので、関数の処理が終了した時に元の状態に戻ってしまう。
つまり、内部変数は関数の処理が実行されている期間だけ、レジスタやスタック上に確保されているだけであり、その処理が終わって値が放棄されてしまった後に参照しようとしても、参照できないというわけである。

## 11. ハードウェアを制御する方法
### 問題
本編に入る前の確認問題
1. アセンブリ言語で、周辺装置と入出力を行う命令は何ですか？
1. I/Oとは、何の略語ですか？
1. 周辺装置を識別する番号を何と呼びますか？
1. IRQとは、何の略語ですか？
1. DMAとは、何の略語ですか？
1. DMAを行う周辺装置を識別するための番号を何と呼びますか？

答え
1. アセンブリ言語で、周辺装置と入出力を行う命令は何ですか？  
IN命令とOUT命令
1. I/Oとは、何の略語ですか？  
Input/Output
1. 周辺装置を識別する番号を何と呼びますか？  
I/OアドレスまたはI/Oポート番号
1. IRQとは、何の略語ですか？  
Interrupt Request
1. DMAとは、何の略語ですか？  
Direct Memory Access
1. DMAを行う周辺装置を識別するための番号を何と呼びますか？  
DMAチャネル

### アプリケーションがハードウェアを制御するには
C言語などの高水準言語を使って何かアプリケーションを作成する場合、ハードウェアを直接制御する命令にお目にかかることは滅多にない。ハードウェアの制御は、Windows（OS）が一手に引き受けれてくれるから。
ただし、アプリケーションからハードウェアを間接的に制御する手段（システムコール）は提供されている。
Windowsの場合はシステムコールのことをAPIと呼ぶ。（この辺りは9章で述べた通り）

### IN命令とOUT命令
Windowsがハードウェアを生業するために使うのは、入出力命令で、その中でも代表的な命令が「IN」と「OUT」の2つ。
これらはアセンブリ言語のニーモニックである。
IN命令は、指定したポート番号のポートからデータを入力し、それをプロセッサ内部のレジスタに格納する。
OUT命令は、プロセッサのレジスタに格納されているデータを、指定したポート番号のポートに出力する。
Pentiumなどの場合、`IN レジスタ名, ポート番号 `、`OUT ポート番号, レジスタ名 `のように記載する。

### ポート／ポート番号
コンピュータ本体には、ディスプレイやキーボードなどの周辺装置を接続するためのコネクタが付いている。
それぞれのコネクタの奥には、コンピュータ本体と周辺装置の電気的特性を相互に変換するICが接続されている。
これらのICを「I/Oコントローラ」と総称する。
電圧の違いや、デジタルとアナログなどの電気的特性の違いがあるため、コンピュータ本体と周辺装置を直接接続することはできない。
そのために、I/Oコントローラが必要になってくる。（I/OはInput/Outputの略）
ディスプレイやキーボードなど周辺装置ごとに専用のI/Oコントローラがある。
I/Oコントローラには、データを一時的に格納しておくための一種のメモリー（精々数バイト）がある。
このメモリーがポートである。
ポート（port）とは「港」という意味で、コンピュータ本体と周辺装置との間で荷物（データ）の積み下ろしをする港のような場所なのでポートと呼ばれている。
I/Oコントローラ内部のメモリーをレジスタと呼ぶこともあるが、マイクロプロセッサの内部のレジスタとは異なるものなので注意。

I/Oコントローラを実現しているICの中には、1個〜数個のポートがある。
コンピュータには複数の周辺装置が接続されているので、複数のI/Oコントローラがあり、複数のポートがある。
1つのI/Oコントローラで1つだけ周辺装置を制御することも、複数の周辺装置を制御することもある。
ポートは、ポート番号を使って識別される。
ポート番号をI/Oアドレスと呼ぶこともある。
IN命令やOUT命令では、ポート番号で指定したポートとマイクロプロセッサの間でデータの入出力を行う。
これは、メモリーアドレスを指定して、メインメモリーを読み書きすることに似ている。

> Linuxでは`/proc/ioports`でI/Oポートアドレスが確認できる。

### IRQ
IRQ（Interrupt Request）とは、割り込み要求という意味で、割り込み処理を行うために必要な仕組みのこと。
割り込み処理とは、現在実行中のプログラムを一旦停止して、他のプログラムに実行を移すこと。
割り込み処理はハードウェアの制御で重要な役割を担っており、割り込み処理がないと処理が円滑に進まない場合がある。

割り込み処理を行うと、割り込んだ側のプログラム（割り込み処理プログラム）の実行が終了するまで、割り込まれたプログラム（メインプログラム）の処理が中断する。
割り込み処理プログラムの実行が終了したら、メインプログラムの処理が再開される。

割り込み処理を要求するのは周辺装置に接続されたI/Oコントローラであり、割り込み処理プログラムを実行するのはプロセッサ。
割り込みの要求を行った周辺機器を特定するには、I/Oのポート番号とは別の番号を使う。
この番号を割り込み番号と呼ぶ。
割り込み番号に応じて実行される割り込み処理プログラムは、OSまたはBIOSが提供してくれる。

もしも複数の周辺装置から同時に割り込みの要求があったら、マイクロプロセッサも困惑してしまう。
そこで、I/Oコントローラとマイクロプロセッサの間に割り込みコントローラというICがワンクッション入るようになっている。
割り込みコントローラは、複数の周辺装置からの割り込みの要求を順番にマイクロプロセッサに伝える。
割り込みコントローラから割り込み要求を受け付けたマイクロプロセッサは、現在実行中のメインプログラムから割り込み処理プログラムに実行を切り替える。
割り込み処理プログラムの最初の処理は、マイクロプロセッサの持つすべてのレジスタの値をメモリー上のスタックに退避すること。
割り込み処理プログラムの中で周辺装置との入出力が完了したら、スタックに退避しておいた値をレジスタに戻し、メインプログラムの処理を続行する。
マイクロプロセッサのレジスタの値を割り込み処理の直前の状態に戻さないと、メインプログラムの実行に影響を与え、最悪の場合プログラムが停止したり暴走してしまう。

> Linuxでは`/proc/interrupts`でIRQの確認ができる。
詳しくはこちら：[/proc/interruptsに関するメモ](http://nbisco.hatenablog.com/entry/2016/12/23/020456)

### 割り込み処理の恩恵
では、メインプログラムの実行中に、どのくらいの頻度で割り込み処理が発生してるかと言うと、ほとんどの周辺装置が頻繁に割り込み要求を行っている。
その理由は、数返送力入力されたデータをリアルタイムで処理してもらうため。
割り込み処理を使わなくても、周辺装置からデータを入力することは可能だが、その場合には、メインプログラムの中で周辺装置からのデータ入力があったかを繰り返し調べ続ける必要がある。
周辺装置は複数あるので、順番に調べる必要がある。
複数の周辺装置の状態を順番に調べることを「ボーリング」と言う。ボーリングは、割り込みがあまり発生しないシステムに適した処理だが、パソコンには不向き。
例えばマウスからの入力の有無を調べている間にキーボードからの入力があった場合、入力された文字がリアルタイムでディスプレイに表示されなくなってしまう。
割り込み処理を行っているからこそ、リアルタイムでパソコンを操作できている。

### DMA
DMA（Direct Memory Access）とは、マイクロプロセッサを仲介させずに周辺装置が直接コンピュータのメインメモリーに書き込みを行うこと。ディスク装置などでDMAが使われている。
これは、ディスクから大量のデータを読み出す際に、マイクロプロセッサにかかる負荷を下げ、他の処理を実行できるようにするために考えられた仕組み。
DMAを採用すると、CPUを経由してメモリーに書き込まなくていいので、大量のデータを短時間にメインメモリーに書き込むことができる。

### 文字やグラフィックスが表示される仕組み
文字やグラフィックスがモニターに表示されるのは、ディスプライに表示される情報を常に記録しているメモリーが存在しているから。
このメモリーのことを「VRAM（Video RAM）」と呼ぶ。
プログラムでVRAMにデータを書き込めば、それがディスプレイに表示される。
そのためのプログラムはOSやBIOSによって提供され、割り込みによって処理される。

MS-DOS時代のパソコンの多くでは、VRAMはメインメモリーの一部となっていた。
そのため、容量が少なく描写力に限界があった（精々16色程度）。
現在のパソコンには、ビデオカードやグラフィックスボードと呼ばれる専用のハードウェア上に、メインメモリーとは独立したVRAMと専用のマイクロプロセッサ（GPU）が装備されているのが一般的。

## 12. コンピュータに「考え」させるためには
この章は機械学習の入門書の方が100倍マシなので読まなくていい。

# 読後の感想
ここまで丁寧にまとめて何だが、まとめるのに時間を書けるなら手を動かしながら繰り返し参照して血肉にすることに時間を割いたほうが良い気がした。
まあこの本に限っては初歩の初歩で知っておくべき情報ばかりだったので、この形でも良かったのかもしれない。
だが、このシリーズの以降の本に関しては、まとめずに読んでいこうと思う。
ただし、ネットワークの本に関してはまとめたほうが良いかも。
